{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## MOSAIKS feature extraction\n",
        "\n",
        "This tutorial demonstrates the **MOSAIKS** method for extracting _feature vectors_ from satellite imagery patches for use in downstream modeling tasks. It will show:\n",
        "- How to extract 1km$^2$ patches of Sentinel 2 or Landsat multispectral imagery for a list of latitude, longitude points\n",
        "- How to extract summary features from each of these imagery patches\n",
        "\n",
        "### Background\n",
        "\n",
        "Consider the case where you have a dataset of latitude and longitude points assosciated with some dependent variable (for example: population density, weather, housing prices, biodiversity) and, potentially, other independent variables. You would like to model the dependent variable as a function of the independent variables, but instead of including latitude and longitude directly in this model, you would like to include some high dimensional representation of what the Earth looks like at that point (that hopefully explains some of the variance in the dependent variable!). From the computer vision literature, there are various [representation learning techniques](https://en.wikipedia.org/wiki/Feature_learning) that can be used to do this, i.e. extract _features vectors_ from imagery. This notebook gives an implementation of the technique described in [Rolf et al. 2021](https://www.nature.com/articles/s41467-021-24638-z), \"A generalizable and accessible approach to machine learning with global satellite imagery\" called Multi-task Observation using Satellite Imagery & Kitchen Sinks (**MOSAIKS**). For more information about **MOSAIKS** see the [project's webpage](http://www.globalpolicy.science/mosaiks).\n",
        "\n",
        "### Environment setup\n",
        "This notebook works with or without an API key, but you will be given more permissive access to the data with an API key.\n",
        "- If you're running this on the [Planetary Computer Hub](http://planetarycomputer.microsoft.com/compute), make sure to choose the **GPU - PyTorch** profile when presented with the form to choose your environment.\n",
        "- The Planetary Computer Hub is pre-configured to use your API key.\n",
        "- To use your API key locally, set the environment variable `PC_SDK_SUBSCRIPTION_KEY` or use `pc.settings.set_subscription_key(<YOUR API Key>)`.\n",
        "    \n",
        "**Notes**:\n",
        "- This example uses either\n",
        "    - [sentinel-2-l2a data](https://planetarycomputer.microsoft.com/dataset/sentinel-2-l2a)\n",
        "    - [landsat-c2-l2 data](https://planetarycomputer.microsoft.com/dataset/landsat-c2-l2)\n",
        "- The techniques used here apply equally well to other remote-sensing datasets."
      ],
      "metadata": {},
      "id": "440e592b-c09d-4e75-91fc-00e5d36d391d"
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install -q git+https://github.com/geopandas/dask-geopandas"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1682705186164
        }
      },
      "id": "522ab90b-af76-477d-a930-4d63c6847028"
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "import time\n",
        "import os\n",
        "import gc\n",
        "import calendar\n",
        "import re\n",
        "\n",
        "RASTERIO_BEST_PRACTICES = dict(  # See https://github.com/pangeo-data/cog-best-practices\n",
        "    CURL_CA_BUNDLE=\"/etc/ssl/certs/ca-certificates.crt\",\n",
        "    GDAL_DISABLE_READDIR_ON_OPEN=\"EMPTY_DIR\",\n",
        "    AWS_NO_SIGN_REQUEST=\"YES\",\n",
        "    GDAL_MAX_RAW_BLOCK_CACHE_SIZE=\"200000000\",\n",
        "    GDAL_SWATH_SIZE=\"200000000\",\n",
        "    VSI_CURL_CACHE_SIZE=\"200000000\",\n",
        ")\n",
        "os.environ.update(RASTERIO_BEST_PRACTICES)\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import rasterio\n",
        "import rasterio.warp\n",
        "import rasterio.mask\n",
        "import shapely.geometry\n",
        "import geopandas\n",
        "import dask_geopandas\n",
        "from dask.distributed import Client, LocalCluster\n",
        "import dask.dataframe as dd\n",
        "import dask_gateway\n",
        "\n",
        "from pystac import Item\n",
        "import stackstac\n",
        "import pyproj\n",
        "\n",
        "warnings.filterwarnings(action=\"ignore\", category=UserWarning, module=\"torch\")\n",
        "warnings.filterwarnings(action=\"ignore\", category=FutureWarning)\n",
        "warnings.filterwarnings(action=\"ignore\", category=RuntimeWarning)\n",
        "warnings.filterwarnings(action=\"ignore\", category=UserWarning)\n",
        "\n",
        "import pystac_client\n",
        "import planetary_computer as pc\n",
        "\n",
        "\n",
        "# Disabling the benchmarking feature with torch.backends.cudnn.benchmark = False \n",
        "# causes cuDNN to deterministically select an algorithm, possibly at the cost of reduced performance.\n",
        "# https://pytorch.org/docs/stable/notes/randomness.html\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "import random\n",
        "random.seed(42)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1682705188778
        }
      },
      "id": "281d0543-f6b0-4b68-a4ba-a99c547b00c8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set Parameters"
      ],
      "metadata": {},
      "id": "3cefa598-0653-4fd4-b7ce-14fb7c4e59e2"
    },
    {
      "cell_type": "code",
      "source": [
        "num_features = 1000\n",
        "country_code = 'ZMB'\n",
        "use_file = False #changed to false \n",
        "# use_file = False"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "collapsed": false,
        "gather": {
          "logged": 1682705189087
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "e28b414c-1d02-491a-9518-1b777898950d"
    },
    {
      "cell_type": "code",
      "source": [
        "#satellite = \"landsat-c2-l2\"\n",
        "#bands = [\n",
        "    #\"red\",\n",
        "    #\"green\", \n",
        "    #\"blue\",\n",
        "    #\"nir08\",\n",
        "    #\"swir16\",\n",
        "    #\"swir22\"\n",
        "#]\n",
        "#bands = [    # Landsat-8 Bands\n",
        "    # \"SR_B1\", # Coastal/Aerosol Band (B1)\n",
        "    #\"SR_B2\", # Blue Band (B2)\n",
        "    #\"SR_B3\", # Green Band (B3)\n",
        "    #\"SR_B4\", # Red Band (B4)\n",
        "    #\"SR_B5\", # Near Infrared Band 0.8 (B5)\n",
        "    #\"SR_B6\", # Short-wave Infrared Band 1.6 (B6)\n",
        "    #\"SR_B7\" # Short-wave Infrared Band 2.2 (B7)\n",
        "#]"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "collapsed": false,
        "gather": {
          "logged": 1682705189377
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "ce354d45-8f23-4630-ae55-95738e4a443d"
    },
    {
      "cell_type": "code",
      "source": [
        " satellite = \"sentinel-2-l2a\"\n",
        " bands = [  # Sentinel-2 Bands\n",
        "     \"B02\", # B02 (blue) 10 meter\n",
        "     \"B03\", # B03 (green) 10 meter\n",
        "#     \"B04\", # B04 (red) 10 meter\n",
        "#     \"B05\", # B05(Veg Red Edge 1) 20 meter\n",
        "#     \"B06\", # B06(Veg Red Edge 2) 20 meter\n",
        "#     \"B07\", # B07(Veg Red Edge 3) 20 meter\n",
        "#     \"B08\", # B08 (NIR) 10 meter\n",
        "#     \"B11\", # B11 (SWIR (1.6)) 20 meter\n",
        "#     \"B12\", # B12 (SWIR (2.2)) 20 meter\n",
        "#     \"visual\", #B04 (red), B03 (green), B02 (blue)\n",
        "]"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "collapsed": false,
        "gather": {
          "logged": 1682705189613
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "b6691b75-c632-448a-845a-33628bf46972"
    },
    {
      "cell_type": "code",
      "source": [
        "if satellite == \"landsat-c2-l2\":\n",
        "    resolution = 30\n",
        "    min_image_edge = 6\n",
        "else:\n",
        "    resolution = 10\n",
        "    min_image_edge = 20"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1682705189844
        }
      },
      "id": "6bb7126f-4aaf-4615-82da-efc06a2140bd"
    },
    {
      "cell_type": "code",
      "source": [
        "channels = len(bands)\n",
        "dat_re = re.compile(r'\\d+') \n",
        "l = [str(int(dat_re.search(x).group())) for x in bands if dat_re.search(x)]\n",
        "bands_short = '-'.join(l)"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "collapsed": false,
        "gather": {
          "logged": 1682705190067
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "860d78aa-ae1d-4650-8ac6-46c345208714"
    },
    {
      "cell_type": "code",
      "source": [
        "#channels = len(bands)\n",
        "#bands_short = \"r-g-b-nir-swir16-swir22\""
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "collapsed": false,
        "gather": {
          "logged": 1682705190288
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "55dc0554-2cbf-4e76-80d3-a7d7a9604f76"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create grid and sample points to featurize"
      ],
      "metadata": {},
      "id": "1ae9b0ec-c334-4751-91c4-91dabff6a44a"
    },
    {
      "cell_type": "code",
      "source": [
        "if use_file:\n",
        "    gdf = pd.read_feather('data/keep/ZMB_crop_weights_20k-points.feather')\n",
        "    gdf = (\n",
        "        geopandas\n",
        "        .GeoDataFrame(\n",
        "            gdf, \n",
        "            geometry = geopandas.points_from_xy(x = gdf.lon, y = gdf.lat), \n",
        "            crs='EPSG:4326')\n",
        "    )\n",
        "else:\n",
        "    cell_size = 0.01  # Roughly 1 km\n",
        "    ### get country shape\n",
        "    zambia_url = 'https://raw.githubusercontent.com/wmgeolab/geoBoundaries/7d63961ccefe39c0a68e28d5929aa9c866572180/releaseData/gbOpen/ZMB/ADM0/geoBoundaries-ZMB-ADM0_simplified.geojson'\n",
        "    zambia = geopandas.read_file(zambia_url)\n",
        "    gdf_sea = geopandas.read_file('sea_boundry_geofiles/sea_boundries.shp', crs='EPSG:4326')\n",
        "    gdf_sea['geometry'] = gdf_sea.geometry.buffer(0)\n",
        "\n",
        "    ### Create grid of points\n",
        "    cell_size = .01  # Very roughly 1 km\n",
        "    xmin, ymin, xmax, ymax = gdf_sea.total_bounds\n",
        "    xs = list(np.arange(xmin, xmax + cell_size, cell_size))\n",
        "    ys = list(np.arange(ymin, ymax + cell_size, cell_size))\n",
        "    def make_cell(x, y, cell_size):\n",
        "        ring = [\n",
        "            (x, y),\n",
        "            (x + cell_size, y),\n",
        "            (x + cell_size, y + cell_size),\n",
        "            (x, y + cell_size)\n",
        "        ]\n",
        "        cell = shapely.geometry.Polygon(ring).centroid\n",
        "        return cell\n"
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1682705190533
        }
      },
      "id": "b09c0e05-9ba6-407e-9260-5f9f00decc18"
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for invalid geometries\n",
        "invalid_geoms = gdf_sea[~gdf_sea.geometry.is_valid]\n",
        "\n",
        "if not invalid_geoms.empty:\n",
        "    print(f\"There are {len(invalid_geoms)} invalid geometries in gdf_sea.\")\n",
        "    # You can then fix or remove the invalid geometries as needed\n",
        "else:\n",
        "    print(\"All geometries in gdf_sea are valid.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "All geometries in gdf_sea are valid.\n"
        }
      ],
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1682705190783
        }
      },
      "id": "56e2503d-6f7f-463c-88df-69b0b2df7b8f"
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1682703284169
        }
      },
      "id": "83a0ca90-f104-453f-bebc-21ab23db67d3"
    },
    {
      "cell_type": "code",
      "source": [
        "# Azure\n",
        "# Create a LocalCluster with default settings\n",
        "cluster = LocalCluster()\n",
        "\n",
        "# Connect a Dask client to the cluster\n",
        "client = Client(cluster)\n",
        "\n",
        "# MPC\n",
        "#cluster = dask_gateway.GatewayCluster()\n",
        "#client = cluster.get_client()\n",
        "#cluster.adapt(minimum=2, maximum=50)\n",
        "#print(cluster.dashboard_link)"
      ],
      "outputs": [],
      "execution_count": 11,
      "metadata": {
        "gather": {
          "logged": 1682705191055
        }
      },
      "id": "323b1751-a461-4331-a5dc-241344bdc73d"
    },
    {
      "cell_type": "code",
      "source": [
        "%time\n",
        "center_points = []\n",
        "for x in xs:\n",
        "    for y in ys:\n",
        "        cell = make_cell(x, y, cell_size)\n",
        "        center_points.append(cell)\n",
        "            \n",
        "    ### Put grid into a GeDataFrame for cropping to country shape\n",
        "gdf = geopandas.GeoDataFrame({'geometry': center_points}, crs = 'EPSG:4326')\n",
        "gdf['lon'], gdf['lat'] = gdf.geometry.x, gdf.geometry.y\n",
        "gdf = gdf.sample(frac=0.01, random_state=43, ignore_index=False) \n",
        "\n",
        "    # convert GeoDataFrame to Dask GeoDataFrame\n",
        "dask_gdf = dask_geopandas.from_geopandas(gdf, npartitions=47)\n",
        "\n",
        "# use Dask to apply the within() method in parallel\n",
        "dask_gdf_within = dask_gdf.map_partitions(\n",
        "    lambda partition: partition[partition.geometry.within(zambia.unary_union)],\n",
        "    meta=gdf.head(0)\n",
        ")\n",
        "\n",
        "# convert Dask GeoDataFrame back to GeoDataFrame\n",
        "gdf = dask_gdf_within.compute()\n",
        "\n",
        "points = gdf[[\"lon\", \"lat\"]].to_numpy()\n",
        "pt_len = gdf.shape[0]\n",
        "gdf.shape"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "CPU times: user 3 µs, sys: 1e+03 ns, total: 4 µs\nWall time: 6.91 µs\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 12,
          "data": {
            "text/plain": "(6186, 3)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 12,
      "metadata": {
        "gather": {
          "logged": 1682705259539
        }
      },
      "id": "c851b193-2ced-4052-8287-8b12db1624d3"
    },
    {
      "cell_type": "code",
      "source": [
        "# Close the Dask client\n",
        "client.close()\n",
        "\n",
        "cluster.close()"
      ],
      "outputs": [],
      "execution_count": 13,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1682705259859
        }
      },
      "id": "0b0e998e-58e0-441d-ade6-04c8420d8e2f"
    },
    {
      "cell_type": "code",
      "source": [
        "#gdf.to_file('sea.geojson', driver = 'GeoJSON')"
      ],
      "outputs": [],
      "execution_count": 14,
      "metadata": {
        "gather": {
          "logged": 1682705260122
        }
      },
      "id": "37b1b57a-e64d-4bb8-a34d-b2e615a92353"
    },
    {
      "cell_type": "code",
      "source": [
        "#gdf = geopandas.read_file('~/PlanetaryComputerExamples/Capstone/Featurization/sea.geojson')\n",
        "#points = gdf[[\"lon\", \"lat\"]].to_numpy()\n",
        "#pt_len = gdf.shape[0]\n",
        "#gdf.shape"
      ],
      "outputs": [],
      "execution_count": 15,
      "metadata": {
        "gather": {
          "logged": 1682705260386
        }
      },
      "id": "078554fe-7c80-4968-bb12-bd0a25c5a437"
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we define the pytorch model that we will use to extract the features and a helper method. The **MOSAIKS** methodology describes several ways to do this and we use the simplest."
      ],
      "metadata": {},
      "id": "10dc75c7-a83e-49eb-ae1b-eb30c40f23d7"
    },
    {
      "cell_type": "code",
      "source": [
        "class RCF(nn.Module):\n",
        "    \"\"\"A model for extracting Random Convolution Features (RCF) from input imagery.\"\"\"\n",
        "    def __init__(self, num_features=16, kernel_size=3, num_input_channels=channels):\n",
        "        super(RCF, self).__init__()\n",
        "        # We create `num_features / 2` filters so require `num_features` to be divisible by 2\n",
        "        assert num_features % 2 == 0, \"Please enter an even number of features.\"\n",
        "        # Applies a 2D convolution over an input image composed of several input planes.\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            num_input_channels,\n",
        "            num_features // 2,\n",
        "            kernel_size=kernel_size,\n",
        "            stride=1,\n",
        "            padding=0,\n",
        "            dilation=1,\n",
        "            bias=True,\n",
        "        )\n",
        "        # Fills the input Tensor 'conv1.weight' with values drawn from the normal distribution\n",
        "        nn.init.normal_(self.conv1.weight, mean=0.0, std=1.0)\n",
        "        # Fills the input Tensor 'conv1.bias' with the value 'val = -1'.\n",
        "        nn.init.constant_(self.conv1.bias, -1.0)\n",
        "    def forward(self, x):\n",
        "        # The rectified linear activation function or ReLU for short is a piecewise linear function \n",
        "        # that will output the input directly if it is positive, otherwise, it will output zero.\n",
        "        x1a = F.relu(self.conv1(x), inplace=True)\n",
        "        # The below step is where we take the inverse which is appended later\n",
        "        x1b = F.relu(-self.conv1(x), inplace=True)\n",
        "        # Applies a 2D adaptive average pooling over an input signal composed of several input planes.\n",
        "        x1a = F.adaptive_avg_pool2d(x1a, (1, 1)).squeeze()\n",
        "        x1b = F.adaptive_avg_pool2d(x1b, (1, 1)).squeeze()\n",
        "        if len(x1a.shape) == 1:  # case where we passed a single input\n",
        "            return torch.cat((x1a, x1b), dim=0)\n",
        "        elif len(x1a.shape) == 2:  # case where we passed a batch of > 1 inputs\n",
        "            return torch.cat((x1a, x1b), dim=1)"
      ],
      "outputs": [],
      "execution_count": 16,
      "metadata": {
        "gather": {
          "logged": 1682705260644
        }
      },
      "id": "600886c1-24cd-4197-aa61-172b1606d33d"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we initialize the model and pytorch components"
      ],
      "metadata": {},
      "id": "c06fbe67-d966-43dd-a2ef-af157487192f"
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\")\n",
        "model = RCF(num_features).eval().to(device)"
      ],
      "outputs": [],
      "execution_count": 17,
      "metadata": {
        "gather": {
          "logged": 1682705260918
        }
      },
      "id": "b244448f-9b79-4c58-b779-5e8a4957c84e"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extract features from the imagery around each point\n",
        "\n",
        "We need to find a suitable Sentinel 2 scene for each point. As usual, we'll use `pystac-client` to search for items matching some conditions, but we don't just want do make a `.search()` call for each of the 67,968 remaining points. Each HTTP request is relatively slow. Instead, we will *batch* or points and search *in parallel*.\n",
        "\n",
        "We need to be a bit careful with how we batch up our points though. Since a single Sentinel 2 scene will cover many points, we want to make sure that points which are spatially close together end up in the same batch. In short, we need to spatially partition the dataset. This is implemented in `dask-geopandas`.\n",
        "\n",
        "So the overall workflow will be\n",
        "\n",
        "1. Find an appropriate STAC item for each point (in parallel, using the spatially partitioned dataset)\n",
        "2. Feed the points and STAC items to a custom Dataset that can read imagery given a point and the URL of a overlapping S2 scene\n",
        "3. Use a custom Dataloader, which uses our Dataset, to feed our model imagery and save the corresponding features"
      ],
      "metadata": {},
      "id": "4ef68b83-ce2e-4e70-908f-3e909337af04"
    },
    {
      "cell_type": "code",
      "source": [
        "NPARTITIONS = 250\n",
        "\n",
        "ddf = dask_geopandas.from_geopandas(gdf, npartitions=1)\n",
        "hd = ddf.hilbert_distance().compute()\n",
        "gdf[\"hd\"] = hd\n",
        "gdf = gdf.sort_values(\"hd\")\n",
        "\n",
        "dgdf = dask_geopandas.from_geopandas(gdf, npartitions=NPARTITIONS, sort=False)"
      ],
      "outputs": [],
      "execution_count": 18,
      "metadata": {
        "gather": {
          "logged": 1682705261255
        },
        "tags": []
      },
      "id": "ebfd0c2e-78fe-45a9-99a4-13112f0da841"
    },
    {
      "cell_type": "code",
      "source": [
        "del ddf\n",
        "del hd\n",
        "del gdf\n",
        "gc.collect()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 19,
          "data": {
            "text/plain": "451"
          },
          "metadata": {}
        }
      ],
      "execution_count": 19,
      "metadata": {
        "gather": {
          "logged": 1682705261514
        }
      },
      "id": "3a38a958-18cb-4a46-afc2-2a7807a00d00"
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "start_month = 9\n",
        "year_start = 2015\n",
        "year_end = 2021\n",
        "\n",
        "buffer_size = 0.000 #do we need a buffer for SEAs?\n",
        "cloud_limit = 20\n",
        "\n",
        "batch_size = 30\n",
        "workers = os.cpu_count()\n",
        "\n",
        "print(\"Using:  \\n\", \n",
        "      f\"  Satellite: {satellite}  \\n\",\n",
        "      f\"  Pixel Resolution: {resolution}  \\n\",\n",
        "      f\"  Grid Resolution: {buffer_size * 2} degree squared (WGS84) \\n\",\n",
        "      f\"  Cloud Limit: less than {cloud_limit}%  \\n\",\n",
        "      f\"  Bands: {bands} \\n\",\n",
        "      f\"  Points: {pt_len} \\n\",\n",
        "      f\"  Number Features: {num_features} features \\n\",\n",
        "      f\"  Year Range: {year_start} to {year_end} \\n\")\n",
        "\n",
        "for yr in range(year_start, year_end+1):\n",
        "    features = pd.DataFrame()\n",
        "    ft = []\n",
        "    \n",
        "    if (yr == year_start):\n",
        "        month_range = range(start_month, 13)\n",
        "    else:\n",
        "        month_range = range(1, 13) \n",
        "        \n",
        "    for mn in month_range:\n",
        "\n",
        "        if mn < 10:\n",
        "            month = \"0\"+str(mn)\n",
        "        else:\n",
        "            month = mn\n",
        "\n",
        "        def query(points):\n",
        "            \"\"\"\n",
        "            Find a STAC item for points in the `points` DataFrame\n",
        "\n",
        "            Parameters\n",
        "            ----------\n",
        "            points : geopandas.GeoDataFrame\n",
        "                A GeoDataFrame\n",
        "\n",
        "            Returns\n",
        "            -------\n",
        "            geopandas.GeoDataFrame\n",
        "                A new geopandas.GeoDataFrame with a `stac_item` column containing the STAC\n",
        "                item that covers each point.\n",
        "            \"\"\"\n",
        "            intersects = shapely.geometry.mapping(points.unary_union.convex_hull)\n",
        "\n",
        "            catalog = pystac_client.Client.open(\n",
        "                \"https://planetarycomputer.microsoft.com/api/stac/v1\"\n",
        "            )\n",
        "            # Define search date range for query\n",
        "            ending_day = calendar.monthrange(yr, int(mn))[1]\n",
        "            search_start = f\"{yr}-{month}-01\" \n",
        "            search_end = f\"{yr}-{month}-{ending_day}\" \n",
        "            \n",
        "            # The time frame in which we search for non-cloudy imagery\n",
        "            search = catalog.search(\n",
        "                collections=[satellite],  \n",
        "                intersects=intersects,\n",
        "                datetime=[search_start, search_end],\n",
        "                query={\"eo:cloud_cover\": {\"lt\": cloud_limit},\n",
        "                      \"platform\": {\"in\": [\"landsat-8\"]}},\n",
        "                limit=500,\n",
        "            )\n",
        "            ic = search.get_all_items_as_dict()\n",
        "            features = ic[\"features\"]\n",
        "            features_d = {item[\"id\"]: item for item in features}\n",
        "            data = {\n",
        "                \"eo:cloud_cover\": [],\n",
        "                \"geometry\": [],\n",
        "            }\n",
        "            index = []\n",
        "            for item in features:\n",
        "                data[\"eo:cloud_cover\"].append(item[\"properties\"][\"eo:cloud_cover\"])\n",
        "                data[\"geometry\"].append(shapely.geometry.shape(item[\"geometry\"]))\n",
        "                index.append(item[\"id\"])\n",
        "            items = geopandas.GeoDataFrame(data, index=index, geometry=\"geometry\").sort_values(\n",
        "                \"eo:cloud_cover\"\n",
        "            )\n",
        "            point_list = points.geometry.tolist()\n",
        "            point_items = []\n",
        "            for point in point_list:\n",
        "                covered_by = items[items.covers(point)]\n",
        "                if len(covered_by):\n",
        "                    point_items.append(features_d[covered_by.index[0]])\n",
        "                else:\n",
        "                    # There weren't any scenes matching our conditions for this point (too cloudy)\n",
        "                    point_items.append(None)\n",
        "            return points.assign(stac_item=point_items)\n",
        "\n",
        "        tic = time.time()\n",
        "        print(\"Matching images to points for: \", mn, \"-\", yr, sep = \"\")\n",
        "\n",
        "        with Client(n_workers=16) as client:\n",
        "            meta = dgdf._meta.assign(stac_item=[])\n",
        "            df2 = dgdf.map_partitions(query, meta=meta).compute()\n",
        "        df3 = df2.dropna(subset=[\"stac_item\"]).reset_index(drop = True)\n",
        "\n",
        "        matching_items = []\n",
        "        for item in df3.stac_item.tolist():\n",
        "            signed_item = pc.sign(Item.from_dict(item))\n",
        "            matching_items.append(signed_item)\n",
        "\n",
        "\n",
        "        points = df3[[\"lon\", \"lat\"]].to_numpy()\n",
        "        \n",
        "        print(\"Found acceptable images for \", \n",
        "              points.shape[0], \"/\", pt_len,\n",
        "              \" points in \", \n",
        "              f\"{time.time()-tic:0.2f} seconds\", \n",
        "              sep = \"\")\n",
        "\n",
        "\n",
        "        class CustomDataset(Dataset):\n",
        "            def __init__(self, points, items, buffer=buffer_size):\n",
        "                self.points = points\n",
        "                self.items = items\n",
        "                self.buffer = buffer\n",
        "\n",
        "            def __len__(self):\n",
        "                return self.points.shape[0]\n",
        "\n",
        "            def __getitem__(self, idx):\n",
        "\n",
        "                lon, lat = self.points[idx]\n",
        "                fn = self.items[idx]\n",
        "\n",
        "                if fn is None:\n",
        "                    return None\n",
        "                else:\n",
        "                    try:\n",
        "                        stack = stackstac.stack(\n",
        "                            fn,\n",
        "                            assets=bands,\n",
        "                            resolution=resolution,\n",
        "                        )\n",
        "                        x_min, y_min = pyproj.Proj(stack.crs)(lon-self.buffer, lat-self.buffer)\n",
        "                        x_max, y_max = pyproj.Proj(stack.crs)(lon+self.buffer, lat+self.buffer)\n",
        "                        aoi = stack.loc[..., y_max:y_min, x_min:x_max]\n",
        "                        data = aoi.compute(\n",
        "                            scheduler=\"single-threaded\"\n",
        "                            )\n",
        "                        out_image = data.data \n",
        "                        out_image = ((out_image - out_image.min()) ) / (out_image.max() - out_image.min())\n",
        "                    except ValueError:\n",
        "                        pass\n",
        "                    out_image = torch.from_numpy(out_image).float()\n",
        "                    return out_image\n",
        "\n",
        "        dataset = CustomDataset(points, matching_items)\n",
        "\n",
        "        dataloader = DataLoader(\n",
        "            dataset,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=False,\n",
        "            num_workers=workers,\n",
        "            collate_fn=lambda x: x,\n",
        "            pin_memory=False,\n",
        "            # persistent_workers=True,\n",
        "        )\n",
        "\n",
        "        x_all = np.zeros((points.shape[0], num_features), dtype=float)\n",
        "        tic = time.time()\n",
        "        toc = time.time()\n",
        "        i = 0\n",
        "        print(\"Featurizing: \", month, \"-\", yr, sep = \"\")\n",
        "        for images in dataloader:\n",
        "            for image in images:\n",
        "\n",
        "                if i % 1000 == 0:\n",
        "                    print(\n",
        "                        f\"{i}/{points.shape[0]} -- {i / points.shape[0] * 100:0.2f}%\"\n",
        "                        + f\" -- {time.time()-tic:0.2f} seconds\"\n",
        "                    )\n",
        "                    tic = time.time()\n",
        "\n",
        "                    # LS 8 scene size is 185 km x 180 km\n",
        "\n",
        "                if image is not None:\n",
        "                    # each image should have dim (time, bands, height, width) so len(image.shape) == 4\n",
        "                    # with only 1 timestamp (image.shape[0] == 1)\n",
        "                    # Ideally an image.shape will be (1, 7, 33, 34)\n",
        "                    assert len(image.shape) == 4, image.shape[0] == 1\n",
        "                    # A full image should be ~33x34 pixels (i.e. ~1km^2 at a 30m/px spatial\n",
        "                    # resolution), however we can receive smaller images if an input point\n",
        "                    # happens to be at the edge of a Landsat scene (a literal edge case). To deal\n",
        "                    # with these (edge) cases we crudely drop all images where the spatial\n",
        "                    # dimensions aren't both greater than 20 pixels.\n",
        "\n",
        "                    # if type(image) == torch.Tensor: \n",
        "                    try:\n",
        "                        if image.shape[2] >= min_image_edge and image.shape[3] >= min_image_edge:\n",
        "                            image = image.to(device)\n",
        "                            with torch.no_grad():\n",
        "                                feats = model(image).cpu().numpy()\n",
        "                            x_all[i] = feats\n",
        "                        else:\n",
        "                            # this happens if the point is close to the edge \n",
        "                            # of a scene (one or both of the spatial dimensions\n",
        "                            # of the image are very small)\n",
        "                            pass\n",
        "                    except ValueError: \n",
        "                        pass \n",
        "                else:\n",
        "                    pass  # this happens if we do not find a S2 scene for some point\n",
        "                i += 1\n",
        "                \n",
        "                \n",
        "                torch.cuda.empty_cache()\n",
        "                \n",
        "            # torch.cuda.empty_cache()\n",
        "                \n",
        "                \n",
        "                \n",
        "                \n",
        "                \n",
        "        features_monthly = pd.DataFrame(x_all)\n",
        "        features_monthly[[\"lon\", \"lat\"]] = points.tolist()\n",
        "        features_monthly['year'] = yr\n",
        "        features_monthly['month'] = mn\n",
        "        \n",
        "        # ft.append(features_monthly)\n",
        "\n",
        "        features_monthly.columns = features_monthly.columns.astype(str)\n",
        "        \n",
        "        # Save the features to a feather file\n",
        "        file_name = (f'data/{satellite}_bands-{bands_short}_{country_code}_{pt_len/1000:.0f}'+\n",
        "                    f'k-points_{num_features}-features_{yr}_{mn}.feather')\n",
        "        \n",
        "        print(\"Saving file as:\", file_name)\n",
        "        features_monthly.to_feather(file_name)\n",
        "        \n",
        "        # Free memory before loop iterates\n",
        "        print(\"Freeing RAM\")\n",
        "        del meta\n",
        "        del query\n",
        "        del df2\n",
        "        del df3\n",
        "        del points\n",
        "        del dataset\n",
        "        del dataloader\n",
        "        del x_all\n",
        "        del features_monthly\n",
        "        del CustomDataset\n",
        "        gc.collect()\n",
        "        print(f\"Done in {(time.time()-toc)/60:0.2f} minutes\")\n",
        "        print('')\n",
        "    # features = pd.concat(ft).reset_index(drop = True)\n",
        "    \n",
        "    # features.columns = features.columns.astype(str)\n",
        "    \n",
        "    # # Save the features to a feather file\n",
        "    # file_name = (f'data/{satellite}_bands-{bands_short}_{country_code}_{pt_len/1000:.0f}'+\n",
        "    #              f'k-points_{num_features}-features_{yr}.feather')\n",
        "    \n",
        "    # print(\"Saving file as:\", file_name)\n",
        "    # features.to_feather(file_name)\n",
        "    \n",
        "    # display(FileLink(file_name))\n",
        "    \n",
        "    print(\"Save finished!\")\n",
        "    # Free memory before loop iterates\n",
        "    print(\"Freeing RAM\")\n",
        "    # del features\n",
        "    # del ft\n",
        "    # gc.collect()\n",
        "    print('')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Using:  \n   Satellite: sentinel-2-l2a  \n   Pixel Resolution: 10  \n   Grid Resolution: 0.0 degree squared (WGS84) \n   Cloud Limit: less than 20%  \n   Bands: ['B02', 'B03'] \n   Points: 6186 \n   Number Features: 1000 features \n   Year Range: 2015 to 2021 \n\nMatching images to points for: 9-2015\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "2023-04-28 18:12:29,377 - distributed.worker - WARNING - Compute Failed\nKey:       ('query-75f0cbded2bec382a3d720c8357eb02b', 99)\nFunction:  subgraph_callable-701357c5-5f72-4d67-ab44-e3f4abae\nargs:      (                          geometry       lon        lat          hd\n150108  POINT (23.59466 -13.00936)  23.59466 -13.009357  1144304029\n151049  POINT (23.60466 -13.00936)  23.60466 -13.009357  1144308599\n145402  POINT (23.54466 -13.01936)  23.54466 -13.019357  1144446992\n185881  POINT (23.97466 -12.85936)  23.97466 -12.859357  1149656491\n184022  POINT (23.95466 -12.62936)  23.95466 -12.629357  1150954818\n192483  POINT (24.04466 -12.70936)  24.04466 -12.709357  1151480895\n189668  POINT (24.01466 -12.62936)  24.01466 -12.629357  1151717766\n193434  POINT (24.05466 -12.60936)  24.05466 -12.609357  1151768294\n197181  POINT (24.09466 -12.77936)  24.09466 -12.779357  1152593818\n199063  POINT (24.11466 -12.77936)  24.11466 -12.779357  1152607398\n195291  POINT (24.07466 -12.85936)  24.07466 -12.859357  1153238093\n197163  POINT (24.09466 -12.95936)  24.09466 -12.959357  1153598416\n202807  POINT (24.15466 -12.97936)  24.15466 -12.979357  1153698041\n174565  POINT (23.85466 -13.09936)  23.85466 -1\nkwargs:    {}\nException: 'APIError(\"<!DOCTYPE html PUBLIC \\'-//W3C//DTD XHTML 1.0 Transitional//EN\\' \\'http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\\'><html xmlns=\\'http://www.w3.org/1999/xhtml\\'><head><meta content=\\'text/html; charset=utf-8\\' http-equiv=\\'content-type\\'/><style type=\\'text/css\\'>body {font-family:Arial; margin-left:40px; }img  { border:0 none; }#content { margin-left: auto; margin-right: auto }#message h2 { font-size: 20px; font-weight: normal; color: #000000; margin: 34px 0px 0px 0px }#message p  { font-size: 13px; color: #000000; margin: 7px 0px 0px0px}#errorref { font-size: 11px; color: #737373; margin-top: 41px }</style><title>Service unavailable</title></head><body><div id=\\'content\\'><div id=\\'message\\'><h2>Our services aren\\'t available right now</h2><p>We\\'re working to restore all services as soon as possible. Please check back soon.</p></div><div id=\\'errorref\\'><span>0bAxMZAAAAAAA5YbHgryASpkbISeWAyl0QU1TMDRFREdFMTkxMAA5MjdhYmZhNi0xOWY2LTRhZjEtYTA5ZC1jOTU5ZDlhMWU2NDQ=</span></div></div></body></html>\")'\n\n"
        },
        {
          "output_type": "error",
          "ename": "APIError",
          "evalue": "<!DOCTYPE html PUBLIC '-//W3C//DTD XHTML 1.0 Transitional//EN' 'http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd'><html xmlns='http://www.w3.org/1999/xhtml'><head><meta content='text/html; charset=utf-8' http-equiv='content-type'/><style type='text/css'>body {font-family:Arial; margin-left:40px; }img  { border:0 none; }#content { margin-left: auto; margin-right: auto }#message h2 { font-size: 20px; font-weight: normal; color: #000000; margin: 34px 0px 0px 0px }#message p  { font-size: 13px; color: #000000; margin: 7px 0px 0px0px}#errorref { font-size: 11px; color: #737373; margin-top: 41px }</style><title>Service unavailable</title></head><body><div id='content'><div id='message'><h2>Our services aren't available right now</h2><p>We're working to restore all services as soon as possible. Please check back soon.</p></div><div id='errorref'><span>0bAxMZAAAAAAA5YbHgryASpkbISeWAyl0QU1TMDRFREdFMTkxMAA5MjdhYmZhNi0xOWY2LTRhZjEtYTA5ZC1jOTU5ZDlhMWU2NDQ=</span></div></div></body></html>",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAPIError\u001b[0m                                  Traceback (most recent call last)",
            "File \u001b[0;32m<timed exec>:102\u001b[0m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/dask/base.py:314\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;124;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \n\u001b[1;32m    293\u001b[0m \u001b[38;5;124;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;124;03m    dask.base.compute\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 314\u001b[0m     (result,) \u001b[38;5;241m=\u001b[39m \u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/dask/base.py:599\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    596\u001b[0m     keys\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_keys__())\n\u001b[1;32m    597\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[0;32m--> 599\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/distributed/client.py:3213\u001b[0m, in \u001b[0;36mClient.get\u001b[0;34m(self, dsk, keys, workers, allow_other_workers, resources, sync, asynchronous, direct, retries, priority, fifo_timeout, actors, **kwargs)\u001b[0m\n\u001b[1;32m   3211\u001b[0m         should_rejoin \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3212\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3213\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgather\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpacked\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43masynchronous\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43masynchronous\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdirect\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3214\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   3215\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m futures\u001b[38;5;241m.\u001b[39mvalues():\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/distributed/client.py:2348\u001b[0m, in \u001b[0;36mClient.gather\u001b[0;34m(self, futures, errors, direct, asynchronous)\u001b[0m\n\u001b[1;32m   2346\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2347\u001b[0m     local_worker \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2348\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msync\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2349\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gather\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2351\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2352\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdirect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_worker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_worker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2354\u001b[0m \u001b[43m    \u001b[49m\u001b[43masynchronous\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43masynchronous\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/distributed/utils.py:349\u001b[0m, in \u001b[0;36mSyncMethodMixin.sync\u001b[0;34m(self, func, asynchronous, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    347\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m future\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 349\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msync\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/distributed/utils.py:416\u001b[0m, in \u001b[0;36msync\u001b[0;34m(loop, func, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error:\n\u001b[1;32m    415\u001b[0m     typ, exc, tb \u001b[38;5;241m=\u001b[39m error\n\u001b[0;32m--> 416\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/distributed/utils.py:389\u001b[0m, in \u001b[0;36msync.<locals>.f\u001b[0;34m()\u001b[0m\n\u001b[1;32m    387\u001b[0m         future \u001b[38;5;241m=\u001b[39m wait_for(future, callback_timeout)\n\u001b[1;32m    388\u001b[0m     future \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(future)\n\u001b[0;32m--> 389\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01myield\u001b[39;00m future\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    391\u001b[0m     error \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mexc_info()\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/tornado/gen.py:769\u001b[0m, in \u001b[0;36mRunner.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    766\u001b[0m exc_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    771\u001b[0m     exc_info \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mexc_info()\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/distributed/client.py:2211\u001b[0m, in \u001b[0;36mClient._gather\u001b[0;34m(self, futures, errors, direct, local_worker)\u001b[0m\n\u001b[1;32m   2209\u001b[0m         exc \u001b[38;5;241m=\u001b[39m CancelledError(key)\n\u001b[1;32m   2210\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2211\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exception\u001b[38;5;241m.\u001b[39mwith_traceback(traceback)\n\u001b[1;32m   2212\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m   2213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/dask/optimization.py:990\u001b[0m, in \u001b[0;36m__call__\u001b[0;34m()\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minkeys):\n\u001b[1;32m    989\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m args, got \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minkeys), \u001b[38;5;28mlen\u001b[39m(args)))\n\u001b[0;32m--> 990\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m core\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdsk, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutkey, \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minkeys, args)))\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/dask/core.py:149\u001b[0m, in \u001b[0;36mget\u001b[0;34m()\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m toposort(dsk):\n\u001b[1;32m    148\u001b[0m     task \u001b[38;5;241m=\u001b[39m dsk[key]\n\u001b[0;32m--> 149\u001b[0m     result \u001b[38;5;241m=\u001b[39m _execute_task(task, cache)\n\u001b[1;32m    150\u001b[0m     cache[key] \u001b[38;5;241m=\u001b[39m result\n\u001b[1;32m    151\u001b[0m result \u001b[38;5;241m=\u001b[39m _execute_task(out, cache)\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/dask/core.py:119\u001b[0m, in \u001b[0;36m_execute_task\u001b[0;34m()\u001b[0m\n\u001b[1;32m    115\u001b[0m     func, args \u001b[38;5;241m=\u001b[39m arg[\u001b[38;5;241m0\u001b[39m], arg[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# Note: Don't assign the subtask results to a variable. numpy detects\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# temporaries by their reference count and can execute certain\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;66;03m# operations in-place.\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m(_execute_task(a, cache) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m args))\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ishashable(arg):\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arg\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/dask/utils.py:73\u001b[0m, in \u001b[0;36mapply\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;03m\"\"\"Apply a function given its positional and keyword arguments.\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \n\u001b[1;32m     44\u001b[0m \u001b[38;5;124;03mEquivalent to ``func(*args, **kwargs)``\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m>>> dsk = {'task-name': task}  # adds the task to a low level Dask task graph\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs)\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/dask/dataframe/core.py:7006\u001b[0m, in \u001b[0;36mapply_and_enforce\u001b[0;34m()\u001b[0m\n\u001b[1;32m   7004\u001b[0m func \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_func\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   7005\u001b[0m meta \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_meta\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 7006\u001b[0m df \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   7007\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_dataframe_like(df) \u001b[38;5;129;01mor\u001b[39;00m is_series_like(df) \u001b[38;5;129;01mor\u001b[39;00m is_index_like(df):\n\u001b[1;32m   7008\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(df):\n",
            "File \u001b[0;32m<timed exec>:71\u001b[0m, in \u001b[0;36mquery\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/pystac_client/item_search.py:860\u001b[0m, in \u001b[0;36mget_all_items_as_dict\u001b[0;34m()\u001b[0m\n\u001b[1;32m    847\u001b[0m \u001b[38;5;124;03m\"\"\"DEPRECATED\u001b[39;00m\n\u001b[1;32m    848\u001b[0m \n\u001b[1;32m    849\u001b[0m \u001b[38;5;124;03m.. deprecated:: 0.4.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    853\u001b[0m \u001b[38;5;124;03m    Dict : A GeoJSON FeatureCollection\u001b[39;00m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    855\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_all_items_as_dict() is deprecated, use item_collection_as_dict() \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstead.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    858\u001b[0m     \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[1;32m    859\u001b[0m )\n\u001b[0;32m--> 860\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem_collection_as_dict()\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/pystac_client/item_search.py:766\u001b[0m, in \u001b[0;36mitem_collection_as_dict\u001b[0;34m()\u001b[0m\n\u001b[1;32m    753\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    754\u001b[0m \u001b[38;5;124;03mGet the matching items as an item-collection-like dict.\u001b[39;00m\n\u001b[1;32m    755\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[38;5;124;03m    Dict : A GeoJSON FeatureCollection\u001b[39;00m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    765\u001b[0m features \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 766\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m page \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stac_io\u001b[38;5;241m.\u001b[39mget_pages(\n\u001b[1;32m    767\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmethod, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_parameters()\n\u001b[1;32m    768\u001b[0m ):\n\u001b[1;32m    769\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m page[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    770\u001b[0m         features\u001b[38;5;241m.\u001b[39mappend(feature)\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/pystac_client/stac_api_io.py:248\u001b[0m, in \u001b[0;36mget_pages\u001b[0;34m()\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_pages\u001b[39m(\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    238\u001b[0m     url: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    239\u001b[0m     method: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    240\u001b[0m     parameters: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Any]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    241\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;124;03m\"\"\"Iterator that yields dictionaries for each page at a STAC paging\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;124;03m    endpoint, e.g., /collections, /search\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \n\u001b[1;32m    245\u001b[0m \u001b[38;5;124;03m    Return:\u001b[39;00m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;124;03m        Dict[str, Any] : JSON content from a single page\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 248\u001b[0m     page \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_json(url, method\u001b[38;5;241m=\u001b[39mmethod, parameters\u001b[38;5;241m=\u001b[39mparameters)\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (page\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m page\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcollections\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[1;32m    250\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/pystac/stac_io.py:202\u001b[0m, in \u001b[0;36mread_json\u001b[0;34m()\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_json\u001b[39m(\u001b[38;5;28mself\u001b[39m, source: HREF, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;124;03m\"\"\"Read a dict from the given source.\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \n\u001b[1;32m    188\u001b[0m \u001b[38;5;124;03m    See :func:`StacIO.read_text <pystac.StacIO.read_text>` for usage of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;124;03m        given source.\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 202\u001b[0m     txt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_text(source, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjson_loads(txt)\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/pystac_client/stac_api_io.py:123\u001b[0m, in \u001b[0;36mread_text\u001b[0;34m()\u001b[0m\n\u001b[1;32m    121\u001b[0m href \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(source)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_url(href):\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(href, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(href) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/pystac_client/stac_api_io.py:171\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m()\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m APIError(\u001b[38;5;28mstr\u001b[39m(err))\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[0;32m--> 171\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m APIError\u001b[38;5;241m.\u001b[39mfrom_response(resp)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mAPIError\u001b[0m: <!DOCTYPE html PUBLIC '-//W3C//DTD XHTML 1.0 Transitional//EN' 'http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd'><html xmlns='http://www.w3.org/1999/xhtml'><head><meta content='text/html; charset=utf-8' http-equiv='content-type'/><style type='text/css'>body {font-family:Arial; margin-left:40px; }img  { border:0 none; }#content { margin-left: auto; margin-right: auto }#message h2 { font-size: 20px; font-weight: normal; color: #000000; margin: 34px 0px 0px 0px }#message p  { font-size: 13px; color: #000000; margin: 7px 0px 0px0px}#errorref { font-size: 11px; color: #737373; margin-top: 41px }</style><title>Service unavailable</title></head><body><div id='content'><div id='message'><h2>Our services aren't available right now</h2><p>We're working to restore all services as soon as possible. Please check back soon.</p></div><div id='errorref'><span>0bAxMZAAAAAAA5YbHgryASpkbISeWAyl0QU1TMDRFREdFMTkxMAA5MjdhYmZhNi0xOWY2LTRhZjEtYTA5ZC1jOTU5ZDlhMWU2NDQ=</span></div></div></body></html>"
          ]
        }
      ],
      "execution_count": 22,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "id": "80e79884-beb9-4df5-a547-28ede40f699b"
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml-pt-tf"
    },
    "kernelspec": {
      "name": "python38-azureml-pt-tf",
      "language": "python",
      "display_name": "Python 3.8 - Pytorch and Tensorflow"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}