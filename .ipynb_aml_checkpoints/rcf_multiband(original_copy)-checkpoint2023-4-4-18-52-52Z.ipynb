{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## MOSAIKS feature extraction\n",
        "\n",
        "This tutorial demonstrates the **MOSAIKS** method for extracting _feature vectors_ from satellite imagery patches for use in downstream modeling tasks. It will show:\n",
        "- How to extract 1km$^2$ patches of Sentinel 2 or Landsat multispectral imagery for a list of latitude, longitude points\n",
        "- How to extract summary features from each of these imagery patches\n",
        "- How to use the summary features in a linear model of the population density at each point\n",
        "\n",
        "### Background\n",
        "\n",
        "Consider the case where you have a dataset of latitude and longitude points assosciated with some dependent variable (for example: population density, weather, housing prices, biodiversity) and, potentially, other independent variables. You would like to model the dependent variable as a function of the independent variables, but instead of including latitude and longitude directly in this model, you would like to include some high dimensional representation of what the Earth looks like at that point (that hopefully explains some of the variance in the dependent variable!). From the computer vision literature, there are various [representation learning techniques](https://en.wikipedia.org/wiki/Feature_learning) that can be used to do this, i.e. extract _features vectors_ from imagery. This notebook gives an implementation of the technique described in [Rolf et al. 2021](https://www.nature.com/articles/s41467-021-24638-z), \"A generalizable and accessible approach to machine learning with global satellite imagery\" called Multi-task Observation using Satellite Imagery & Kitchen Sinks (**MOSAIKS**). For more information about **MOSAIKS** see the [project's webpage](http://www.globalpolicy.science/mosaiks).\n",
        "\n",
        "### Environment setup\n",
        "This notebook works with or without an API key, but you will be given more permissive access to the data with an API key.\n",
        "- If you're running this on the [Planetary Computer Hub](http://planetarycomputer.microsoft.com/compute), make sure to choose the **GPU - PyTorch** profile when presented with the form to choose your environment.\n",
        "- The Planetary Computer Hub is pre-configured to use your API key.\n",
        "- To use your API key locally, set the environment variable `PC_SDK_SUBSCRIPTION_KEY` or use `pc.settings.set_subscription_key(<YOUR API Key>)`.\n",
        "    \n",
        "**Notes**:\n",
        "- This example uses either\n",
        "    - [sentinel-2-l2a data](https://planetarycomputer.microsoft.com/dataset/sentinel-2-l2a)\n",
        "    - [landsat-c2-l2 data](https://planetarycomputer.microsoft.com/dataset/landsat-c2-l2)\n",
        "- The techniques used here apply equally well to other remote-sensing datasets."
      ],
      "metadata": {},
      "id": "440e592b-c09d-4e75-91fc-00e5d36d391d"
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install -q git+https://github.com/geopandas/dask-geopandas"
      ],
      "outputs": [],
      "execution_count": 19,
      "metadata": {
        "gather": {
          "logged": 1683173198894
        }
      },
      "id": "522ab90b-af76-477d-a930-4d63c6847028"
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "import time\n",
        "import os\n",
        "import gc\n",
        "import calendar\n",
        "import re\n",
        "\n",
        "RASTERIO_BEST_PRACTICES = dict(  # See https://github.com/pangeo-data/cog-best-practices\n",
        "    CURL_CA_BUNDLE=\"/etc/ssl/certs/ca-certificates.crt\",\n",
        "    GDAL_DISABLE_READDIR_ON_OPEN=\"EMPTY_DIR\",\n",
        "    AWS_NO_SIGN_REQUEST=\"YES\",\n",
        "    GDAL_MAX_RAW_BLOCK_CACHE_SIZE=\"200000000\",\n",
        "    GDAL_SWATH_SIZE=\"200000000\",\n",
        "    VSI_CURL_CACHE_SIZE=\"200000000\",\n",
        ")\n",
        "os.environ.update(RASTERIO_BEST_PRACTICES)\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import rasterio\n",
        "import rasterio.warp\n",
        "import rasterio.mask\n",
        "import shapely.geometry\n",
        "import geopandas\n",
        "import dask_geopandas\n",
        "from dask.distributed import Client, LocalCluster\n",
        "\n",
        "from pystac import Item\n",
        "import stackstac\n",
        "import pyproj\n",
        "\n",
        "warnings.filterwarnings(action=\"ignore\", category=UserWarning, module=\"torch\")\n",
        "warnings.filterwarnings(action=\"ignore\", category=FutureWarning)\n",
        "warnings.filterwarnings(action=\"ignore\", category=RuntimeWarning)\n",
        "warnings.filterwarnings(action=\"ignore\", category=UserWarning)\n",
        "\n",
        "import pystac_client\n",
        "import planetary_computer as pc\n",
        "\n",
        "\n",
        "# Disabling the benchmarking feature with torch.backends.cudnn.benchmark = False \n",
        "# causes cuDNN to deterministically select an algorithm, possibly at the cost of reduced performance.\n",
        "# https://pytorch.org/docs/stable/notes/randomness.html\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "import random\n",
        "random.seed(42)"
      ],
      "outputs": [],
      "execution_count": 20,
      "metadata": {
        "gather": {
          "logged": 1683173199041
        }
      },
      "id": "281d0543-f6b0-4b68-a4ba-a99c547b00c8"
    },
    {
      "cell_type": "code",
      "source": [
        "pc.settings.set_subscription_key('5415ad8c61524e0aaa97d0bb5fb45fb7')"
      ],
      "outputs": [],
      "execution_count": 21,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1683173199188
        }
      },
      "id": "cf3a5afc-a866-432a-9923-2924810f6a26"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set Parameters"
      ],
      "metadata": {},
      "id": "3cefa598-0653-4fd4-b7ce-14fb7c4e59e2"
    },
    {
      "cell_type": "code",
      "source": [
        "num_features = 750\n",
        "country_code = 'ZMB'\n",
        "use_file = True\n",
        "#use_file = False"
      ],
      "outputs": [],
      "execution_count": 22,
      "metadata": {
        "collapsed": false,
        "gather": {
          "logged": 1683173199337
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "e28b414c-1d02-491a-9518-1b777898950d"
    },
    {
      "cell_type": "code",
      "source": [
        "# satellite = \"landsat-c2-l2\"\n",
        "# bands = [\n",
        "#     \"red\",\n",
        "#     \"green\", \n",
        "#     \"blue\",\n",
        "#     \"nir08\",\n",
        "#     \"swir16\",\n",
        "#     \"swir22\"\n",
        "# ]"
      ],
      "outputs": [],
      "execution_count": 23,
      "metadata": {
        "collapsed": false,
        "gather": {
          "logged": 1683173199476
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "ce354d45-8f23-4630-ae55-95738e4a443d"
    },
    {
      "cell_type": "code",
      "source": [
        "satellite = \"sentinel-2-l2a\"\n",
        "bands = [ # Sentinel-2 Bands\n",
        "     \"B02\", # B02 (blue) 10 meter\n",
        "     \"B03\", # B03 (green) 10 meter\n",
        "     \"B04\", # B04 (red) 10 meter\n",
        "#     \"B05\", # B05(Veg Red Edge 1) 20 meter\n",
        "#     \"B06\", # B06(Veg Red Edge 2) 20 meter\n",
        "#     \"B07\", # B07(Veg Red Edge 3) 20 meter\n",
        "     \"B08\", # B08 (NIR) 10 meter\n",
        "#     \"B11\", # B11 (SWIR (1.6)) 20 meter\n",
        "#     \"B12\", # B12 (SWIR (2.2)) 20 meter\n",
        " ]"
      ],
      "outputs": [],
      "execution_count": 24,
      "metadata": {
        "collapsed": false,
        "gather": {
          "logged": 1683173199590
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "b6691b75-c632-448a-845a-33628bf46972"
    },
    {
      "cell_type": "code",
      "source": [
        "bands"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 25,
          "data": {
            "text/plain": "['B02', 'B03', 'B04', 'B08']"
          },
          "metadata": {}
        }
      ],
      "execution_count": 25,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1683173199718
        }
      },
      "id": "36c272cb-99b2-4fcb-8f8c-6bb377c9af18"
    },
    {
      "cell_type": "code",
      "source": [
        "if satellite == \"landsat-c2-l2\":\n",
        "    resolution = 30\n",
        "    min_image_edge = 6\n",
        "else:\n",
        "    resolution = 10\n",
        "    min_image_edge = 20"
      ],
      "outputs": [],
      "execution_count": 26,
      "metadata": {
        "gather": {
          "logged": 1683173199837
        }
      },
      "id": "6bb7126f-4aaf-4615-82da-efc06a2140bd"
    },
    {
      "cell_type": "code",
      "source": [
        " dat_re = re.compile(r'\\d+') \n",
        " l = [str(int(dat_re.search(x).group())) for x in bands if dat_re.search(x)]\n",
        " bands_short = '-'.join(l)"
      ],
      "outputs": [],
      "execution_count": 27,
      "metadata": {
        "collapsed": false,
        "gather": {
          "logged": 1683173199943
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "860d78aa-ae1d-4650-8ac6-46c345208714"
    },
    {
      "cell_type": "code",
      "source": [
        "channels = len(bands)\n",
        "#bands_short = \"r-g-b-nir-swir16-swir22\""
      ],
      "outputs": [],
      "execution_count": 28,
      "metadata": {
        "collapsed": false,
        "gather": {
          "logged": 1683173200049
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "55dc0554-2cbf-4e76-80d3-a7d7a9604f76"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create grid and sample points to featurize"
      ],
      "metadata": {},
      "id": "1ae9b0ec-c334-4751-91c4-91dabff6a44a"
    },
    {
      "cell_type": "code",
      "source": [
        "# Azure\n",
        "# # Create a LocalCluster with default settings\n",
        "# cluster = LocalCluster()\n",
        "\n",
        "# # Connect a Dask client to the cluster\n",
        "# client = Client(cluster)\n",
        "\n",
        "# # MPC\n",
        "# #cluster = dask_gateway.GatewayCluster()\n",
        "# #client = cluster.get_client()\n",
        "# #cluster.adapt(minimum=2, maximum=50)\n",
        "# #print(cluster.dashboard_link)"
      ],
      "outputs": [],
      "execution_count": 29,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1683173200155
        }
      },
      "id": "39904830-ec88-418c-b59f-2acdf6867c3b"
    },
    {
      "cell_type": "code",
      "source": [
        "if use_file:\n",
        "    gdf = geopandas.read_file('sea_points.geojson')\n",
        "    gdf = (\n",
        "        geopandas\n",
        "        .GeoDataFrame(\n",
        "            gdf, \n",
        "            geometry = geopandas.points_from_xy(x = gdf.lon, y = gdf.lat), \n",
        "            crs='EPSG:4326')\n",
        "    )\n",
        "else:\n",
        "    cell_size = 0.01  # Roughly 1 km\n",
        "    ### get country shape\n",
        "    zambia_url = 'https://raw.githubusercontent.com/wmgeolab/geoBoundaries/7d63961ccefe39c0a68e28d5929aa9c866572180/releaseData/gbOpen/ZMB/ADM0/geoBoundaries-ZMB-ADM0_simplified.geojson'\n",
        "    country = geopandas.read_file(zambia_url)\n",
        "    #### This would be simpler, but throws an error down the line if used \n",
        "    # world = geopandas.read_file(geopandas.datasets.get_path('naturalearth_lowres'))\n",
        "    # country = world.query(f'iso_a3 == \"{country_code}\"')\n",
        "    ### Create grid of points\n",
        "    cell_size = .01  # Very roughly 1 km\n",
        "    xmin, ymin, xmax, ymax = country.total_bounds\n",
        "    xs = list(np.arange(xmin, xmax + cell_size, cell_size))\n",
        "    ys = list(np.arange(ymin, ymax + cell_size, cell_size))\n",
        "    def make_cell(x, y, cell_size):\n",
        "        ring = [\n",
        "            (x, y),\n",
        "            (x + cell_size, y),\n",
        "            (x + cell_size, y + cell_size),\n",
        "            (x, y + cell_size)\n",
        "        ]\n",
        "        cell = shapely.geometry.Polygon(ring).centroid\n",
        "        return cell\n",
        "    center_points = []\n",
        "    for x in xs:\n",
        "        for y in ys:\n",
        "            cell = make_cell(x, y, cell_size)\n",
        "            center_points.append(cell)\n",
        "    ### Put grid into a GeDataFrame for cropping to country shape\n",
        "    gdf = geopandas.GeoDataFrame({'geometry': center_points}, crs = 'EPSG:4326')\n",
        "    gdf['lon'], gdf['lat'] = gdf.geometry.x, gdf.geometry.y\n",
        "    ### Subset to country \n",
        "    ### This buffer ensures that no points are take at the border \n",
        "    ### which would lead to duplication with neighboring countries\n",
        "    gdf = gdf[gdf.within(country.unary_union)]\n",
        "    gdf = gdf[['lon', 'lat', 'geometry']].reset_index(drop = True)\n",
        "    gdf = gdf.sample(frac = 0.1, random_state=42, ignore_index=False)\n",
        "    points = gdf[[\"lon\", \"lat\"]].to_numpy()\n",
        "pt_len = gdf.shape[0]\n",
        "gdf.shape"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 30,
          "data": {
            "text/plain": "(8447, 3)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 30,
      "metadata": {
        "gather": {
          "logged": 1683173200283
        }
      },
      "id": "b09c0e05-9ba6-407e-9260-5f9f00decc18"
    },
    {
      "cell_type": "code",
      "source": [
        "# # Close the client\n",
        "# client.close()\n",
        "\n",
        "# # Close the cluster\n",
        "# cluster.close()\n",
        "\n",
        "# #MPC\n",
        "# #cluster.close()"
      ],
      "outputs": [],
      "execution_count": 31,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1683173200417
        }
      },
      "id": "46c0ba31-420e-486d-bb0c-12f99c91d483"
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we define the pytorch model that we will use to extract the features and a helper method. The **MOSAIKS** methodology describes several ways to do this and we use the simplest."
      ],
      "metadata": {},
      "id": "10dc75c7-a83e-49eb-ae1b-eb30c40f23d7"
    },
    {
      "cell_type": "code",
      "source": [
        "class RCF(nn.Module):\n",
        "    \"\"\"A model for extracting Random Convolution Features (RCF) from input imagery.\"\"\"\n",
        "    def __init__(self, num_features=16, kernel_size=3, num_input_channels=channels):\n",
        "        super(RCF, self).__init__()\n",
        "        # We create `num_features / 2` filters so require `num_features` to be divisible by 2\n",
        "        assert num_features % 2 == 0, \"Please enter an even number of features.\"\n",
        "        # Applies a 2D convolution over an input image composed of several input planes.\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            num_input_channels,\n",
        "            num_features // 2,\n",
        "            kernel_size=kernel_size,\n",
        "            stride=1,\n",
        "            padding=0,\n",
        "            dilation=1,\n",
        "            bias=True,\n",
        "        )\n",
        "        # Fills the input Tensor 'conv1.weight' with values drawn from the normal distribution\n",
        "        nn.init.normal_(self.conv1.weight, mean=0.0, std=1.0)\n",
        "        # Fills the input Tensor 'conv1.bias' with the value 'val = -1'.\n",
        "        nn.init.constant_(self.conv1.bias, -1.0)\n",
        "    def forward(self, x):\n",
        "        # The rectified linear activation function or ReLU for short is a piecewise linear function \n",
        "        # that will output the input directly if it is positive, otherwise, it will output zero.\n",
        "        x1a = F.relu(self.conv1(x), inplace=True)\n",
        "        # The below step is where we take the inverse which is appended later\n",
        "        x1b = F.relu(-self.conv1(x), inplace=True)\n",
        "        # Applies a 2D adaptive average pooling over an input signal composed of several input planes.\n",
        "        x1a = F.adaptive_avg_pool2d(x1a, (1, 1)).squeeze()\n",
        "        x1b = F.adaptive_avg_pool2d(x1b, (1, 1)).squeeze()\n",
        "        if len(x1a.shape) == 1:  # case where we passed a single input\n",
        "            return torch.cat((x1a, x1b), dim=0)\n",
        "        elif len(x1a.shape) == 2:  # case where we passed a batch of > 1 inputs\n",
        "            return torch.cat((x1a, x1b), dim=1)"
      ],
      "outputs": [],
      "execution_count": 32,
      "metadata": {
        "gather": {
          "logged": 1683173200524
        }
      },
      "id": "600886c1-24cd-4197-aa61-172b1606d33d"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we initialize the model and pytorch components"
      ],
      "metadata": {},
      "id": "c06fbe67-d966-43dd-a2ef-af157487192f"
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\")\n",
        "model = RCF(num_features).eval().to(device)"
      ],
      "outputs": [],
      "execution_count": 33,
      "metadata": {
        "gather": {
          "logged": 1683173200629
        }
      },
      "id": "b244448f-9b79-4c58-b779-5e8a4957c84e"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extract features from the imagery around each point\n",
        "\n",
        "We need to find a suitable Sentinel 2 scene for each point. As usual, we'll use `pystac-client` to search for items matching some conditions, but we don't just want do make a `.search()` call for each of the 67,968 remaining points. Each HTTP request is relatively slow. Instead, we will *batch* or points and search *in parallel*.\n",
        "\n",
        "We need to be a bit careful with how we batch up our points though. Since a single Sentinel 2 scene will cover many points, we want to make sure that points which are spatially close together end up in the same batch. In short, we need to spatially partition the dataset. This is implemented in `dask-geopandas`.\n",
        "\n",
        "So the overall workflow will be\n",
        "\n",
        "1. Find an appropriate STAC item for each point (in parallel, using the spatially partitioned dataset)\n",
        "2. Feed the points and STAC items to a custom Dataset that can read imagery given a point and the URL of a overlapping S2 scene\n",
        "3. Use a custom Dataloader, which uses our Dataset, to feed our model imagery and save the corresponding features"
      ],
      "metadata": {},
      "id": "4ef68b83-ce2e-4e70-908f-3e909337af04"
    },
    {
      "cell_type": "code",
      "source": [
        "NPARTITIONS = 250\n",
        "\n",
        "ddf = dask_geopandas.from_geopandas(gdf, npartitions=1)\n",
        "hd = ddf.hilbert_distance().compute()\n",
        "gdf[\"hd\"] = hd\n",
        "gdf = gdf.sort_values(\"hd\")\n",
        "\n",
        "dgdf = dask_geopandas.from_geopandas(gdf, npartitions=NPARTITIONS, sort=False)\n",
        "\n",
        "del ddf\n",
        "del hd\n",
        "del gdf\n",
        "gc.collect()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 34,
          "data": {
            "text/plain": "1050"
          },
          "metadata": {}
        }
      ],
      "execution_count": 34,
      "metadata": {
        "gather": {
          "logged": 1683173200761
        },
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "id": "ebfd0c2e-78fe-45a9-99a4-13112f0da841"
    },
    {
      "cell_type": "code",
      "source": [
        "%whos"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Variable                  Type            Data/Info\n---------------------------------------------------\nClient                    type            <class 'distributed.client.Client'>\nDataLoader                type            <class 'torch.utils.data.dataloader.DataLoader'>\nDataset                   type            <class 'torch.utils.data.dataset.Dataset'>\nF                         module          <module 'torch.nn.functio<...>/torch/nn/functional.py'>\nItem                      ABCMeta         <class 'pystac.item.Item'>\nLocalCluster              type            <class 'distributed.deploy.local.LocalCluster'>\nNPARTITIONS               int             250\nRASTERIO_BEST_PRACTICES   dict            n=6\nRCF                       type            <class '__main__.RCF'>\nbands                     list            n=4\nbands_short               str             2-3-4-8\nbatch_size                int             15\nbuffer_size               float           0.005\ncalendar                  module          <module 'calendar' from '<...>b/python3.8/calendar.py'>\nchannels                  int             4\nclient                    Client          <Client: No scheduler connected>\ncloud_limit               int             20\ncountry_code              str             ZMB\ndask_geopandas            module          <module 'dask_geopandas' <...>k_geopandas/__init__.py'>\ndat_re                    Pattern         re.compile('\\\\d+')\ndevice                    device          cuda\ndgdf                      GeoDataFrame    Dask GeoDataFrame Structu<...>rom_pandas, 1 graph layer\nfeats                     ndarray         750: 750 elems, type `float32`, 3000 bytes\nfeatures                  DataFrame       Empty DataFrame\\nColumns: []\\nIndex: []\nfile_name                 str             data/sentinel-2-l2a_bands<...>-features_2016_11.feather\nft                        list            n=0\ngc                        module          <module 'gc' (built-in)>\ngeopandas                 module          <module 'geopandas' from <...>s/geopandas/__init__.py'>\ni                         int             7657\nimage                     Tensor          tensor([[[[0.1584, 0.1490<...>\\n       device='cuda:0')\nimages                    list            n=7\nitem                      dict            n=10\nl                         list            n=4\nmatching_items            list            n=7657\nmin_image_edge            int             20\nmn                        int             11\nmodel                     RCF             RCF(\\n  (conv1): Conv2d(4<...>(3, 3), stride=(1, 1))\\n)\nmonth                     int             11\nmonth_range               range           range(11, 13)\nnn                        module          <module 'torch.nn' from '<...>es/torch/nn/__init__.py'>\nnp                        module          <module 'numpy' from '/an<...>kages/numpy/__init__.py'>\nnum_features              int             750\npc                        module          <module 'planetary_comput<...>ry_computer/__init__.py'>\npd                        module          <module 'pandas' from '/a<...>ages/pandas/__init__.py'>\nplt                       module          <module 'matplotlib.pyplo<...>es/matplotlib/pyplot.py'>\npt_len                    int             8447\npyproj                    module          <module 'pyproj' from '/a<...>ages/pyproj/__init__.py'>\npystac_client             module          <module 'pystac_client' f<...>stac_client/__init__.py'>\nrandom                    module          <module 'random' from '/a<...>lib/python3.8/random.py'>\nrasterio                  module          <module 'rasterio' from '<...>es/rasterio/__init__.py'>\nresolution                int             10\nsatellite                 str             sentinel-2-l2a\nshapely                   module          <module 'shapely' from '/<...>ges/shapely/__init__.py'>\nsigned_item               Item            <Item id=S2A_MSIL2A_20161<...>5_T35KNB_20210413T221315>\nstackstac                 module          <module 'stackstac' from <...>s/stackstac/__init__.py'>\nstart_month               int             11\ntic                       float           1683173874.5820694\ntime                      module          <module 'time' (built-in)>\ntoc                       float           1683173511.9698167\ntorch                     module          <module 'torch' from '/an<...>kages/torch/__init__.py'>\nuse_file                  bool            True\nwarnings                  module          <module 'warnings' from '<...>b/python3.8/warnings.py'>\nworkers                   int             11\nyear_end                  int             2022\nyear_start                int             2016\nyr                        int             2016\n"
        }
      ],
      "execution_count": 38,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1683173200882
        }
      },
      "id": "807af143-3008-4e64-8529-b12881e1c1fb"
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "start_month = 12\n",
        "year_start = 2017\n",
        "year_end = 2022\n",
        "\n",
        "buffer_size = 0.005\n",
        "cloud_limit = 20\n",
        "\n",
        "batch_size = 15\n",
        "workers = os.cpu_count() -1\n",
        "\n",
        "print(\"Using:  \\n\", \n",
        "      f\"  Satellite: {satellite}  \\n\",\n",
        "      f\"  Pixel Resolution: {resolution}  \\n\",\n",
        "      f\"  Grid Resolution: {buffer_size * 2} degree squared (WGS84) \\n\",\n",
        "      f\"  Cloud Limit: less than {cloud_limit}%  \\n\",\n",
        "      f\"  Bands: {bands} \\n\",\n",
        "      f\"  Points: {pt_len} \\n\",\n",
        "      f\"  Number Features: {num_features} features \\n\",\n",
        "      f\"  Year Range: {year_start} to {year_end} \\n\")\n",
        "\n",
        "for yr in range(year_start, year_end+1):\n",
        "    \n",
        "    if (yr == year_start):\n",
        "        month_range = range(start_month, 13)\n",
        "    else:\n",
        "        month_range = range(1, 13) \n",
        "        \n",
        "    for mn in month_range:\n",
        "\n",
        "        features = pd.DataFrame()\n",
        "        ft = []\n",
        "\n",
        "        if mn < 10:\n",
        "            month = \"0\"+str(mn)\n",
        "        else:\n",
        "            month = mn\n",
        "\n",
        "        def query(points):\n",
        "            \"\"\"\n",
        "            Find a STAC item for points in the `points` DataFrame\n",
        "\n",
        "            Parameters\n",
        "            ----------\n",
        "            points : geopandas.GeoDataFrame\n",
        "                A GeoDataFrame\n",
        "\n",
        "            Returns\n",
        "            -------\n",
        "            geopandas.GeoDataFrame\n",
        "                A new geopandas.GeoDataFrame with a `stac_item` column containing the STAC\n",
        "                item that covers each point.\n",
        "            \"\"\"\n",
        "            intersects = shapely.geometry.mapping(points.unary_union.convex_hull)\n",
        "\n",
        "            catalog = pystac_client.Client.open(\n",
        "                \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n",
        "                modifier=pc.sign_inplace\n",
        "            )\n",
        "            # Define search date range for query\n",
        "            ending_day = calendar.monthrange(yr, int(mn))[1]\n",
        "            search_start = f\"{yr}-{month}-01\" \n",
        "            search_end = f\"{yr}-{month}-{ending_day}\" \n",
        "            \n",
        "            # The time frame in which we search for non-cloudy imagery\n",
        "            search = catalog.search(\n",
        "                collections=[satellite],  \n",
        "                intersects=intersects,\n",
        "                datetime=[search_start, search_end],\n",
        "                query={\"eo:cloud_cover\": {\"lt\": cloud_limit}},\n",
        "                limit=500,\n",
        "            )\n",
        "            ic = search.get_all_items_as_dict()\n",
        "            features = ic[\"features\"]\n",
        "            features_d = {item[\"id\"]: item for item in features}\n",
        "            data = {\n",
        "                \"eo:cloud_cover\": [],\n",
        "                \"geometry\": [],\n",
        "            }\n",
        "            index = []\n",
        "            for item in features:\n",
        "                data[\"eo:cloud_cover\"].append(item[\"properties\"][\"eo:cloud_cover\"])\n",
        "                data[\"geometry\"].append(shapely.geometry.shape(item[\"geometry\"]))\n",
        "                index.append(item[\"id\"])\n",
        "            items = geopandas.GeoDataFrame(data, index=index, geometry=\"geometry\").sort_values(\n",
        "                \"eo:cloud_cover\"\n",
        "            )\n",
        "            point_list = points.geometry.tolist()\n",
        "            point_items = []\n",
        "            for point in point_list:\n",
        "                covered_by = items[items.covers(point)]\n",
        "                if len(covered_by):\n",
        "                    point_items.append(features_d[covered_by.index[0]])\n",
        "                else:\n",
        "                    # There weren't any scenes matching our conditions for this point (too cloudy)\n",
        "                    point_items.append(None)\n",
        "            return points.assign(stac_item=point_items)\n",
        "\n",
        "        tic = time.time()\n",
        "        print(\"Matching images to points for: \", mn, \"-\", yr, sep = \"\")\n",
        "\n",
        "        with Client(n_workers=16) as client:\n",
        "            meta = dgdf._meta.assign(stac_item=[])\n",
        "            df2 = dgdf.map_partitions(query, meta=meta).compute()\n",
        "        df3 = df2.dropna(subset=[\"stac_item\"]).reset_index(drop = True)\n",
        "\n",
        "        matching_items = []\n",
        "        for item in df3.stac_item.tolist():\n",
        "            signed_item = pc.sign(Item.from_dict(item))\n",
        "            matching_items.append(signed_item)\n",
        "\n",
        "\n",
        "        points = df3[[\"lon\", \"lat\"]].to_numpy()\n",
        "        \n",
        "        print(\"Found acceptable images for \", \n",
        "              points.shape[0], \"/\", pt_len,\n",
        "              \" points in \", \n",
        "              f\"{time.time()-tic:0.2f} seconds\", \n",
        "              sep = \"\")\n",
        "\n",
        "\n",
        "        class CustomDataset(Dataset):\n",
        "            def __init__(self, points, items, buffer=buffer_size):\n",
        "                self.points = points\n",
        "                self.items = items\n",
        "                self.buffer = buffer\n",
        "\n",
        "            def __len__(self):\n",
        "                return self.points.shape[0]\n",
        "\n",
        "            def __getitem__(self, idx):\n",
        "\n",
        "                lon, lat = self.points[idx]\n",
        "                fn = self.items[idx]\n",
        "\n",
        "                if fn is None:\n",
        "                    return None\n",
        "                else:\n",
        "                    try:\n",
        "                        stack = stackstac.stack(\n",
        "                            fn,\n",
        "                            assets=bands,\n",
        "                            resolution=resolution,\n",
        "                        )\n",
        "                        x_min, y_min = pyproj.Proj(stack.crs)(lon-self.buffer, lat-self.buffer)\n",
        "                        x_max, y_max = pyproj.Proj(stack.crs)(lon+self.buffer, lat+self.buffer)\n",
        "                        aoi = stack.loc[..., y_max:y_min, x_min:x_max]\n",
        "                        data = aoi.compute(\n",
        "                            scheduler=\"single-threaded\"\n",
        "                            )\n",
        "                        out_image = data.data \n",
        "                        out_image = ((out_image - out_image.min()) ) / (out_image.max() - out_image.min())\n",
        "                    except ValueError:\n",
        "                        pass\n",
        "                    out_image = torch.from_numpy(out_image).float()\n",
        "                    return out_image\n",
        "\n",
        "        dataset = CustomDataset(points, matching_items)\n",
        "\n",
        "        dataloader = DataLoader(\n",
        "            dataset,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=False,\n",
        "            num_workers=workers,\n",
        "            collate_fn=lambda x: x,\n",
        "            pin_memory=False,\n",
        "            persistent_workers=True,\n",
        "        )\n",
        "\n",
        "        x_all = np.zeros((points.shape[0], num_features), dtype=float)\n",
        "        tic = time.time()\n",
        "        toc = time.time()\n",
        "        i = 0\n",
        "        print(\"Featurizing: \", month, \"-\", yr, sep = \"\")\n",
        "        for images in dataloader:\n",
        "            for image in images:\n",
        "\n",
        "                if i % 1000 == 0:\n",
        "                    print(\n",
        "                        f\"{i}/{points.shape[0]} -- {i / points.shape[0] * 100:0.2f}%\"\n",
        "                        + f\" -- {time.time()-tic:0.2f} seconds\"\n",
        "                    )\n",
        "                    tic = time.time()\n",
        "\n",
        "                    # LS 8 scene size is 185 km x 180 km\n",
        "\n",
        "                if image is not None:\n",
        "                    # each image should have dim (time, bands, height, width) so len(image.shape) == 4\n",
        "                    # with only 1 timestamp (image.shape[0] == 1)\n",
        "                    # Ideally an image.shape will be (1, 7, 33, 34)\n",
        "                    assert len(image.shape) == 4, image.shape[0] == 1\n",
        "                    # A full image should be ~33x34 pixels (i.e. ~1km^2 at a 30m/px spatial\n",
        "                    # resolution), however we can receive smaller images if an input point\n",
        "                    # happens to be at the edge of a Landsat scene (a literal edge case). To deal\n",
        "                    # with these (edge) cases we crudely drop all images where the spatial\n",
        "                    # dimensions aren't both greater than 20 pixels.\n",
        "\n",
        "                    # if type(image) == torch.Tensor: \n",
        "                    try:\n",
        "                        if image.shape[2] >= min_image_edge and image.shape[3] >= min_image_edge:\n",
        "                            image = image.to(device)\n",
        "                            with torch.no_grad():\n",
        "                                feats = model(image).cpu().numpy()\n",
        "                            x_all[i] = feats\n",
        "                        else:\n",
        "                            # this happens if the point is close to the edge \n",
        "                            # of a scene (one or both of the spatial dimensions\n",
        "                            # of the image are very small)\n",
        "                            pass\n",
        "                    except ValueError: \n",
        "                        pass \n",
        "                else:\n",
        "                    pass  # this happens if we do not find a S2 scene for some point\n",
        "                i += 1\n",
        "                \n",
        "                \n",
        "                torch.cuda.empty_cache()\n",
        "                \n",
        "            # torch.cuda.empty_cache()\n",
        "                \n",
        "                \n",
        "                \n",
        "                \n",
        "                \n",
        "        features_monthly = pd.DataFrame(x_all)\n",
        "        features_monthly[[\"lon\", \"lat\"]] = points.tolist()\n",
        "        features_monthly['year'] = yr\n",
        "        features_monthly['month'] = mn\n",
        "        \n",
        "        # ft.append(features_monthly)\n",
        "\n",
        "        features_monthly.columns = features_monthly.columns.astype(str)\n",
        "        \n",
        "        # Save the features to a feather file\n",
        "        file_name = (f'data/{satellite}_bands-{bands_short}_{country_code}_{pt_len/1000:.0f}'+\n",
        "                    f'k-points_{num_features}-features_{yr}_{mn}.feather')\n",
        "        \n",
        "        print(\"Saving file as:\", file_name)\n",
        "        features_monthly.to_feather(file_name)\n",
        "        \n",
        "        # Free memory before loop iterates\n",
        "        print(\"Freeing RAM\")\n",
        "        del meta\n",
        "        del query\n",
        "        del df2\n",
        "        del df3\n",
        "        del points\n",
        "        del dataset\n",
        "        del dataloader\n",
        "        del x_all\n",
        "        del features_monthly\n",
        "        del CustomDataset\n",
        "        del ft\n",
        "        del features\n",
        "        gc.collect()\n",
        "        print(f\"Done in {(time.time()-toc)/60:0.2f} minutes\")\n",
        "        print('')\n",
        "    # features = pd.concat(ft).reset_index(drop = True)\n",
        "    \n",
        "    # features.columns = features.columns.astype(str)\n",
        "    \n",
        "    # # Save the features to a feather file\n",
        "    # file_name = (f'data/{satellite}_bands-{bands_short}_{country_code}_{pt_len/1000:.0f}'+\n",
        "    #              f'k-points_{num_features}-features_{yr}.feather')\n",
        "    \n",
        "    # print(\"Saving file as:\", file_name)\n",
        "    # features.to_feather(file_name)\n",
        "    \n",
        "    # display(FileLink(file_name))\n",
        "    \n",
        "    print(\"Save finished!\")\n",
        "    # Free memory before loop iterates\n",
        "    print(\"Freeing RAM\")\n",
        "    # del features\n",
        "    # del ft\n",
        "    # gc.collect()\n",
        "    print('')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Using:  \n   Satellite: sentinel-2-l2a  \n   Pixel Resolution: 10  \n   Grid Resolution: 0.01 degree squared (WGS84) \n   Cloud Limit: less than 20%  \n   Bands: ['B02', 'B03', 'B04', 'B08'] \n   Points: 8447 \n   Number Features: 750 features \n   Year Range: 2017 to 2022 \n\nMatching images to points for: 12-2017\nFound acceptable images for 4388/8447 points in 19.59 seconds\nFeaturizing: 12-2017\n0/4388 -- 0.00% -- 7.27 seconds\n1000/4388 -- 22.79% -- 45.69 seconds\n2000/4388 -- 45.58% -- 31.12 seconds\n3000/4388 -- 68.37% -- 37.89 seconds\n4000/4388 -- 91.16% -- 37.04 seconds\nSaving file as: data/sentinel-2-l2a_bands-2-3-4-8_ZMB_8k-points_750-features_2017_12.feather\nFreeing RAM\nDone in 3.03 minutes\n\nSave finished!\nFreeing RAM\n\nMatching images to points for: 1-2018\nFound acceptable images for 7146/8447 points in 58.59 seconds\nFeaturizing: 01-2018\n0/7146 -- 0.00% -- 9.60 seconds\n1000/7146 -- 13.99% -- 38.23 seconds\n2000/7146 -- 27.99% -- 73.84 seconds\n3000/7146 -- 41.98% -- 29.95 seconds\n4000/7146 -- 55.98% -- 42.90 seconds\n5000/7146 -- 69.97% -- 37.75 seconds\n6000/7146 -- 83.96% -- 42.51 seconds\n7000/7146 -- 97.96% -- 35.68 seconds\nSaving file as: data/sentinel-2-l2a_bands-2-3-4-8_ZMB_8k-points_750-features_2018_1.feather\nFreeing RAM\nDone in 5.28 minutes\n\nMatching images to points for: 2-2018\nFound acceptable images for 4787/8447 points in 50.67 seconds\nFeaturizing: 02-2018\n0/4787 -- 0.00% -- 6.29 seconds\n1000/4787 -- 20.89% -- 58.41 seconds\n2000/4787 -- 41.78% -- 85.76 seconds\n3000/4787 -- 62.67% -- 43.74 seconds\n4000/4787 -- 83.56% -- 35.01 seconds\nSaving file as: data/sentinel-2-l2a_bands-2-3-4-8_ZMB_8k-points_750-features_2018_2.feather\nFreeing RAM\nDone in 4.28 minutes\n\nMatching images to points for: 3-2018\nFound acceptable images for 6749/8447 points in 30.95 seconds\nFeaturizing: 03-2018\n0/6749 -- 0.00% -- 5.19 seconds\n1000/6749 -- 14.82% -- 47.90 seconds\n2000/6749 -- 29.63% -- 42.43 seconds\n3000/6749 -- 44.45% -- 40.81 seconds\n4000/6749 -- 59.27% -- 35.89 seconds\n5000/6749 -- 74.09% -- 58.39 seconds\n6000/6749 -- 88.90% -- 46.95 seconds\nSaving file as: data/sentinel-2-l2a_bands-2-3-4-8_ZMB_8k-points_750-features_2018_3.feather\nFreeing RAM\nDone in 5.07 minutes\n\nMatching images to points for: 4-2018\nFound acceptable images for 8338/8447 points in 31.69 seconds\nFeaturizing: 04-2018\n0/8338 -- 0.00% -- 9.12 seconds\n1000/8338 -- 11.99% -- 51.12 seconds\n2000/8338 -- 23.99% -- 56.18 seconds\n3000/8338 -- 35.98% -- 55.05 seconds\n4000/8338 -- 47.97% -- 42.25 seconds\n5000/8338 -- 59.97% -- 63.18 seconds\n6000/8338 -- 71.96% -- 59.45 seconds\n7000/8338 -- 83.95% -- 83.38 seconds\n8000/8338 -- 95.95% -- 73.03 seconds\nSaving file as: data/sentinel-2-l2a_bands-2-3-4-8_ZMB_8k-points_750-features_2018_4.feather\nFreeing RAM\nDone in 8.53 minutes\n\nMatching images to points for: 5-2018\nFound acceptable images for 8447/8447 points in 35.52 seconds\nFeaturizing: 05-2018\n0/8447 -- 0.00% -- 13.68 seconds\n1000/8447 -- 11.84% -- 57.32 seconds\n2000/8447 -- 23.68% -- 59.09 seconds\n3000/8447 -- 35.52% -- 54.58 seconds\n4000/8447 -- 47.35% -- 98.64 seconds\n5000/8447 -- 59.19% -- 76.25 seconds\n6000/8447 -- 71.03% -- 52.59 seconds\n7000/8447 -- 82.87% -- 56.24 seconds\n8000/8447 -- 94.71% -- 117.88 seconds\nSaving file as: data/sentinel-2-l2a_bands-2-3-4-8_ZMB_8k-points_750-features_2018_5.feather\nFreeing RAM\nDone in 10.02 minutes\n\nMatching images to points for: 6-2018\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "2023-05-04 05:14:15,741 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'tcp://127.0.0.1:45445'.\n2023-05-04 05:14:15,748 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'tcp://127.0.0.1:38815'.\n2023-05-04 05:14:15,749 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'tcp://127.0.0.1:38865'.\nProcess Dask Worker process (from Nanny):\nTraceback (most recent call last):\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/distributed/process.py\", line 202, in _run\n    target(*args, **kwargs)\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/distributed/nanny.py\", line 990, in _run\n    asyncio.run(run())\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/asyncio/runners.py\", line 43, in run\n    return loop.run_until_complete(main)\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/asyncio/base_events.py\", line 603, in run_until_complete\n    self.run_forever()\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n    self._run_once()\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/asyncio/base_events.py\", line 1816, in _run_once\n    if self._ready or self._stopping:\nKeyboardInterrupt\nProcess Dask Worker process (from Nanny):\nTraceback (most recent call last):\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/distributed/process.py\", line 202, in _run\n    target(*args, **kwargs)\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/distributed/nanny.py\", line 990, in _run\n    asyncio.run(run())\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/asyncio/runners.py\", line 43, in run\n    return loop.run_until_complete(main)\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/asyncio/base_events.py\", line 603, in run_until_complete\n    self.run_forever()\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n    self._run_once()\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/asyncio/base_events.py\", line 1823, in _run_once\n    event_list = self._selector.select(timeout)\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/selectors.py\", line 468, in select\n    fd_event_list = self._selector.poll(timeout, max_ev)\nKeyboardInterrupt\nProcess Dask Worker process (from Nanny):\nTraceback (most recent call last):\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/distributed/process.py\", line 202, in _run\n    target(*args, **kwargs)\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/distributed/nanny.py\", line 990, in _run\n    asyncio.run(run())\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/asyncio/runners.py\", line 43, in run\n    return loop.run_until_complete(main)\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/asyncio/base_events.py\", line 603, in run_until_complete\n    self.run_forever()\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n    self._run_once()\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/asyncio/base_events.py\", line 1823, in _run_once\n    event_list = self._selector.select(timeout)\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/selectors.py\", line 468, in select\n    fd_event_list = self._selector.poll(timeout, max_ev)\nKeyboardInterrupt\nProcess Dask Worker process (from Nanny):\nTraceback (most recent call last):\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/distributed/process.py\", line 202, in _run\n    target(*args, **kwargs)\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/distributed/nanny.py\", line 990, in _run\n    asyncio.run(run())\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/asyncio/runners.py\", line 43, in run\n    return loop.run_until_complete(main)\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/asyncio/base_events.py\", line 603, in run_until_complete\n    self.run_forever()\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n    self._run_once()\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/asyncio/base_events.py\", line 1823, in _run_once\n    event_list = self._selector.select(timeout)\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/selectors.py\", line 468, in select\n    fd_event_list = self._selector.poll(timeout, max_ev)\nKeyboardInterrupt\nProcess Dask Worker process (from Nanny):\nTraceback (most recent call last):\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/distributed/process.py\", line 202, in _run\n    target(*args, **kwargs)\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/distributed/nanny.py\", line 990, in _run\n    asyncio.run(run())\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/asyncio/runners.py\", line 43, in run\n    return loop.run_until_complete(main)\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/asyncio/base_events.py\", line 603, in run_until_complete\n    self.run_forever()\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n    self._run_once()\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/asyncio/base_events.py\", line 1823, in _run_once\n    event_list = self._selector.select(timeout)\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/selectors.py\", line 468, in select\n    fd_event_list = self._selector.poll(timeout, max_ev)\nKeyboardInterrupt\nProcess Dask Worker process (from Nanny):\nTraceback (most recent call last):\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/distributed/process.py\", line 202, in _run\n    target(*args, **kwargs)\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/distributed/nanny.py\", line 990, in _run\n    asyncio.run(run())\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/asyncio/runners.py\", line 43, in run\n    return loop.run_until_complete(main)\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/asyncio/base_events.py\", line 603, in run_until_complete\n    self.run_forever()\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n    self._run_once()\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/asyncio/base_events.py\", line 1823, in _run_once\n    event_list = self._selector.select(timeout)\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/selectors.py\", line 468, in select\n    fd_event_list = self._selector.poll(timeout, max_ev)\nKeyboardInterrupt\nProcess Dask Worker process (from Nanny):\nTraceback (most recent call last):\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/distributed/process.py\", line 202, in _run\n    target(*args, **kwargs)\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/distributed/nanny.py\", line 990, in _run\n    asyncio.run(run())\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/asyncio/runners.py\", line 43, in run\n    return loop.run_until_complete(main)\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/asyncio/base_events.py\", line 603, in run_until_complete\n    self.run_forever()\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n    self._run_once()\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/asyncio/base_events.py\", line 1823, in _run_once\n    event_list = self._selector.select(timeout)\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/selectors.py\", line 468, in select\n    fd_event_list = self._selector.poll(timeout, max_ev)\nKeyboardInterrupt\nProcess Dask Worker process (from Nanny):\nTraceback (most recent call last):\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/distributed/process.py\", line 202, in _run\n    target(*args, **kwargs)\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/distributed/nanny.py\", line 990, in _run\n    asyncio.run(run())\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/asyncio/runners.py\", line 43, in run\n    return loop.run_until_complete(main)\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/asyncio/base_events.py\", line 603, in run_until_complete\n    self.run_forever()\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n    self._run_once()\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/asyncio/base_events.py\", line 1823, in _run_once\n    event_list = self._selector.select(timeout)\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/selectors.py\", line 468, in select\n    fd_event_list = self._selector.poll(timeout, max_ev)\nKeyboardInterrupt\nProcess Dask Worker process (from Nanny):\nTraceback (most recent call last):\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/distributed/process.py\", line 202, in _run\n    target(*args, **kwargs)\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/distributed/nanny.py\", line 990, in _run\n    asyncio.run(run())\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/asyncio/runners.py\", line 43, in run\n    return loop.run_until_complete(main)\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/asyncio/base_events.py\", line 603, in run_until_complete\n    self.run_forever()\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n    self._run_once()\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/asyncio/base_events.py\", line 1823, in _run_once\n    event_list = self._selector.select(timeout)\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/selectors.py\", line 468, in select\n    fd_event_list = self._selector.poll(timeout, max_ev)\nKeyboardInterrupt\nProcess Dask Worker process (from Nanny):\nTraceback (most recent call last):\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/distributed/process.py\", line 202, in _run\n    target(*args, **kwargs)\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/distributed/nanny.py\", line 990, in _run\n    asyncio.run(run())\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/asyncio/runners.py\", line 43, in run\n    return loop.run_until_complete(main)\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/asyncio/base_events.py\", line 603, in run_until_complete\n    self.run_forever()\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n    self._run_once()\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/asyncio/base_events.py\", line 1823, in _run_once\n    event_list = self._selector.select(timeout)\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/selectors.py\", line 468, in select\n    fd_event_list = self._selector.poll(timeout, max_ev)\nKeyboardInterrupt\nProcess Dask Worker process (from Nanny):\nTraceback (most recent call last):\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/distributed/process.py\", line 202, in _run\n    target(*args, **kwargs)\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/distributed/nanny.py\", line 990, in _run\n    asyncio.run(run())\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/asyncio/runners.py\", line 43, in run\n    return loop.run_until_complete(main)\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/asyncio/base_events.py\", line 603, in run_until_complete\n    self.run_forever()\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n    self._run_once()\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/asyncio/base_events.py\", line 1823, in _run_once\n    event_list = self._selector.select(timeout)\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/selectors.py\", line 468, in select\n    fd_event_list = self._selector.poll(timeout, max_ev)\nKeyboardInterrupt\nProcess Dask Worker process (from Nanny):\nTraceback (most recent call last):\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/distributed/process.py\", line 202, in _run\n    target(*args, **kwargs)\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/distributed/nanny.py\", line 990, in _run\n    asyncio.run(run())\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/asyncio/runners.py\", line 43, in run\n    return loop.run_until_complete(main)\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/asyncio/base_events.py\", line 603, in run_until_complete\n    self.run_forever()\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n    self._run_once()\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/asyncio/base_events.py\", line 1823, in _run_once\n    event_list = self._selector.select(timeout)\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/selectors.py\", line 468, in select\n    fd_event_list = self._selector.poll(timeout, max_ev)\nKeyboardInterrupt\nProcess Dask Worker process (from Nanny):\nTraceback (most recent call last):\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/distributed/process.py\", line 202, in _run\n    target(*args, **kwargs)\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/distributed/nanny.py\", line 990, in _run\n    asyncio.run(run())\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/asyncio/runners.py\", line 43, in run\n    return loop.run_until_complete(main)\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/asyncio/base_events.py\", line 603, in run_until_complete\n    self.run_forever()\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n    self._run_once()\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/asyncio/base_events.py\", line 1823, in _run_once\n    event_list = self._selector.select(timeout)\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/selectors.py\", line 468, in select\n    fd_event_list = self._selector.poll(timeout, max_ev)\nKeyboardInterrupt\nProcess Dask Worker process (from Nanny):\nTraceback (most recent call last):\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/distributed/process.py\", line 202, in _run\n    target(*args, **kwargs)\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/distributed/nanny.py\", line 990, in _run\n    asyncio.run(run())\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/asyncio/runners.py\", line 43, in run\n    return loop.run_until_complete(main)\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/asyncio/base_events.py\", line 603, in run_until_complete\n    self.run_forever()\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n    self._run_once()\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/asyncio/base_events.py\", line 1823, in _run_once\n    event_list = self._selector.select(timeout)\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/selectors.py\", line 468, in select\n    fd_event_list = self._selector.poll(timeout, max_ev)\nKeyboardInterrupt\nProcess Dask Worker process (from Nanny):\nTraceback (most recent call last):\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/distributed/process.py\", line 202, in _run\n    target(*args, **kwargs)\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/distributed/nanny.py\", line 990, in _run\n    asyncio.run(run())\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/asyncio/runners.py\", line 43, in run\n    return loop.run_until_complete(main)\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/asyncio/base_events.py\", line 603, in run_until_complete\n    self.run_forever()\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n    self._run_once()\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/asyncio/base_events.py\", line 1823, in _run_once\n    event_list = self._selector.select(timeout)\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/selectors.py\", line 468, in select\n    fd_event_list = self._selector.poll(timeout, max_ev)\nKeyboardInterrupt\nProcess Dask Worker process (from Nanny):\nTraceback (most recent call last):\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/distributed/process.py\", line 202, in _run\n    target(*args, **kwargs)\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/distributed/nanny.py\", line 990, in _run\n    asyncio.run(run())\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/asyncio/runners.py\", line 43, in run\n    return loop.run_until_complete(main)\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/asyncio/base_events.py\", line 603, in run_until_complete\n    self.run_forever()\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n    self._run_once()\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/asyncio/base_events.py\", line 1823, in _run_once\n    event_list = self._selector.select(timeout)\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/selectors.py\", line 468, in select\n    fd_event_list = self._selector.poll(timeout, max_ev)\nKeyboardInterrupt\n"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "File \u001b[0;32m<timed exec>:103\u001b[0m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/dask/base.py:314\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;124;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \n\u001b[1;32m    293\u001b[0m \u001b[38;5;124;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;124;03m    dask.base.compute\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 314\u001b[0m     (result,) \u001b[38;5;241m=\u001b[39m \u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/dask/base.py:600\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    597\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[1;32m    599\u001b[0m results \u001b[38;5;241m=\u001b[39m schedule(dsk, keys, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 600\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/dask/base.py:600\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    597\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[1;32m    599\u001b[0m results \u001b[38;5;241m=\u001b[39m schedule(dsk, keys, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 600\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/dask/dataframe/core.py:196\u001b[0m, in \u001b[0;36mfinalize\u001b[0;34m(results)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfinalize\u001b[39m(results):\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_concat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/dask/dataframe/core.py:191\u001b[0m, in \u001b[0;36m_concat\u001b[0;34m(args, ignore_index)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;66;03m# We filter out empty partitions here because pandas frequently has\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;66;03m# inconsistent dtypes in results between empty and non-empty frames.\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m# Ideally this would be handled locally for each operation, but in practice\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# this seems easier. TODO: don't do this.\u001b[39;00m\n\u001b[1;32m    187\u001b[0m args2 \u001b[38;5;241m=\u001b[39m [i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m args \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(i)]\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    189\u001b[0m     args[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args2\n\u001b[0;32m--> 191\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mmethods\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muniform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m )\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/dask/dataframe/dispatch.py:64\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(dfs, axis, join, uniform, filter_warning, ignore_index, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     63\u001b[0m     func \u001b[38;5;241m=\u001b[39m concat_dispatch\u001b[38;5;241m.\u001b[39mdispatch(\u001b[38;5;28mtype\u001b[39m(dfs[\u001b[38;5;241m0\u001b[39m]))\n\u001b[0;32m---> 64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdfs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[43muniform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muniform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilter_warning\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilter_warning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/dask/dataframe/backends.py:655\u001b[0m, in \u001b[0;36mconcat_pandas\u001b[0;34m(dfs, axis, join, uniform, filter_warning, ignore_index, **kwargs)\u001b[0m\n\u001b[1;32m    653\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m filter_warning:\n\u001b[1;32m    654\u001b[0m                 warnings\u001b[38;5;241m.\u001b[39msimplefilter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mFutureWarning\u001b[39;00m)\n\u001b[0;32m--> 655\u001b[0m             out \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdfs3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    657\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_categorical_dtype(dfs2[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdtype):\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/pandas/core/reshape/concat.py:385\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    370\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    372\u001b[0m op \u001b[38;5;241m=\u001b[39m _Concatenator(\n\u001b[1;32m    373\u001b[0m     objs,\n\u001b[1;32m    374\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m     sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[1;32m    383\u001b[0m )\n\u001b[0;32m--> 385\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/pandas/core/reshape/concat.py:616\u001b[0m, in \u001b[0;36m_Concatenator.get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    612\u001b[0m             indexers[ax] \u001b[38;5;241m=\u001b[39m obj_labels\u001b[38;5;241m.\u001b[39mget_indexer(new_labels)\n\u001b[1;32m    614\u001b[0m     mgrs_indexers\u001b[38;5;241m.\u001b[39mappend((obj\u001b[38;5;241m.\u001b[39m_mgr, indexers))\n\u001b[0;32m--> 616\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[43mconcatenate_managers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmgrs_indexers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_axes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcat_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbm_axis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_copy_on_write():\n\u001b[1;32m    620\u001b[0m     new_data\u001b[38;5;241m.\u001b[39m_consolidate_inplace()\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/pandas/core/internals/concat.py:234\u001b[0m, in \u001b[0;36mconcatenate_managers\u001b[0;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[1;32m    231\u001b[0m     values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(vals, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;66;03m# TODO(EA2D): special-casing not needed with 2D EAs\u001b[39;00m\n\u001b[0;32m--> 234\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43mconcat_compat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m     values \u001b[38;5;241m=\u001b[39m ensure_block_shape(values, ndim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    237\u001b[0m values \u001b[38;5;241m=\u001b[39m ensure_wrapped_if_datetimelike(values)\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/pandas/core/dtypes/concat.py:97\u001b[0m, in \u001b[0;36mconcat_compat\u001b[0;34m(to_concat, axis, ea_compat_axis)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(to_concat[\u001b[38;5;241m0\u001b[39m], ABCExtensionArray):\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;66;03m# TODO: what about EA-backed Index?\u001b[39;00m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(to_concat[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m---> 97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_concat_same_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mto_concat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mconcatenate(to_concat)\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/geopandas/array.py:1323\u001b[0m, in \u001b[0;36mGeometryArray._concat_same_type\u001b[0;34m(cls, to_concat)\u001b[0m\n\u001b[1;32m   1311\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;124;03mConcatenate multiple array\u001b[39;00m\n\u001b[1;32m   1313\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[38;5;124;03mExtensionArray\u001b[39;00m\n\u001b[1;32m   1321\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1322\u001b[0m data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([ga\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;28;01mfor\u001b[39;00m ga \u001b[38;5;129;01min\u001b[39;00m to_concat])\n\u001b[0;32m-> 1323\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m GeometryArray(data, crs\u001b[38;5;241m=\u001b[39m\u001b[43m_get_common_crs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mto_concat\u001b[49m\u001b[43m)\u001b[49m)\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/geopandas/array.py:1398\u001b[0m, in \u001b[0;36m_get_common_crs\u001b[0;34m(arr_seq)\u001b[0m\n\u001b[1;32m   1396\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_common_crs\u001b[39m(arr_seq):\n\u001b[0;32m-> 1398\u001b[0m     crs_set \u001b[38;5;241m=\u001b[39m {arr\u001b[38;5;241m.\u001b[39mcrs \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m arr_seq}\n\u001b[1;32m   1399\u001b[0m     crs_not_none \u001b[38;5;241m=\u001b[39m [crs \u001b[38;5;28;01mfor\u001b[39;00m crs \u001b[38;5;129;01min\u001b[39;00m crs_set \u001b[38;5;28;01mif\u001b[39;00m crs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m   1400\u001b[0m     names \u001b[38;5;241m=\u001b[39m [crs\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m crs \u001b[38;5;129;01min\u001b[39;00m crs_not_none]\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/geopandas/array.py:1398\u001b[0m, in \u001b[0;36m<setcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1396\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_common_crs\u001b[39m(arr_seq):\n\u001b[0;32m-> 1398\u001b[0m     crs_set \u001b[38;5;241m=\u001b[39m {arr\u001b[38;5;241m.\u001b[39mcrs \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m arr_seq}\n\u001b[1;32m   1399\u001b[0m     crs_not_none \u001b[38;5;241m=\u001b[39m [crs \u001b[38;5;28;01mfor\u001b[39;00m crs \u001b[38;5;129;01min\u001b[39;00m crs_set \u001b[38;5;28;01mif\u001b[39;00m crs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m   1400\u001b[0m     names \u001b[38;5;241m=\u001b[39m [crs\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m crs \u001b[38;5;129;01min\u001b[39;00m crs_not_none]\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/pyproj/crs/crs.py:1553\u001b[0m, in \u001b[0;36mCRS.__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__hash__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mhash\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_wkt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/pyproj/crs/crs.py:1225\u001b[0m, in \u001b[0;36mCRS.to_wkt\u001b[0;34m(self, version, pretty)\u001b[0m\n\u001b[1;32m   1196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_wkt\u001b[39m(\n\u001b[1;32m   1197\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1198\u001b[0m     version: Union[WktVersion, \u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m WktVersion\u001b[38;5;241m.\u001b[39mWKT2_2019,\n\u001b[1;32m   1199\u001b[0m     pretty: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1200\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m   1201\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1202\u001b[0m \u001b[38;5;124;03m    Convert the projection to a WKT string.\u001b[39;00m\n\u001b[1;32m   1203\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1223\u001b[0m \u001b[38;5;124;03m    str\u001b[39;00m\n\u001b[1;32m   1224\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1225\u001b[0m     wkt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_crs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_wkt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mversion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mversion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretty\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1226\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m wkt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1227\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m CRSError(\n\u001b[1;32m   1228\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCRS cannot be converted to a WKT string of a \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mversion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m version. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1229\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSelect a different version of a WKT string or edit your CRS.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1230\u001b[0m         )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "execution_count": 40,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "id": "80e79884-beb9-4df5-a547-28ede40f699b"
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml-pt-tf"
    },
    "kernelspec": {
      "name": "python38-azureml-pt-tf",
      "language": "python",
      "display_name": "Python 3.8 - Pytorch and Tensorflow"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}